{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Doa8NdMop1cX"
      },
      "source": [
        "# **LLM Fine Tunning**\n",
        "**This notebook demonstrates a comprehensive workflow for finetuning a Qwen25-1.5B-Instruct model for Arabic news using LoRA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb3yrU-iIeLS"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SxWrRPQH7k0",
        "outputId": "da8570cf-2a15-4f71-c00d-382bd3225728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "klXHVVc6p1cb",
        "outputId": "70948c69-ccc9-4842-fdf6-b52dbff158c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 348, done.\u001b[K\n",
            "remote: Counting objects: 100% (348/348), done.\u001b[K\n",
            "remote: Compressing objects: 100% (289/289), done.\u001b[K\n",
            "remote: Total 348 (delta 83), reused 146 (delta 44), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (348/348), 9.53 MiB | 19.28 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting datasets<=3.4.1,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
            "Requirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers<=0.21.0,>=0.19.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting gradio<=5.21.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
            "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.11.0)\n",
            "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Collecting fire (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic (from llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting av (from llamafactory==0.9.3.dev0)\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.14)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Collecting ffmpy (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
            "Collecting pydub (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26070 sha256=f9f6936a5547655cd3b8d793072d51d42f91bdcbde6097dd82dd08ce22d546a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g71lsaar/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=310dbd2b065b0a58d9cf659aaa7aecaac611d9c8674df2f0e9557c6b848a5118\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, pydantic-core, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, markupsafe, groovy, fsspec, fire, ffmpy, dill, av, aiofiles, tiktoken, starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, tokenizers, sse-starlette, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, transformers, gradio, datasets, trl, llamafactory\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.0\n",
            "    Uninstalling pydantic_core-2.33.0:\n",
            "      Successfully uninstalled pydantic_core-2.33.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.0\n",
            "    Uninstalling pydantic-2.11.0:\n",
            "      Successfully uninstalled pydantic-2.11.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.2\n",
            "    Uninstalling transformers-4.50.2:\n",
            "      Successfully uninstalled transformers-4.50.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 av-14.2.0 datasets-3.4.1 dill-0.3.8 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.12.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 llamafactory-0.9.3.dev0 markupsafe-2.1.5 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.46.1 tiktoken-0.9.0 tokenizers-0.21.0 tomlkit-0.13.2 transformers-4.50.0 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qviYWLUhxDE6",
        "outputId": "50f16dd8-517e-4c24-907f-24a6c84ae3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: transformers 4.50.0\n",
            "Uninstalling transformers-4.50.0:\n",
            "  Successfully uninstalled transformers-4.50.0\n",
            "\u001b[33mWARNING: Skipping vllm as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llamafactory 0.9.3.dev0 requires transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2; python_version >= \"3.10\" and sys_platform != \"darwin\", which is not installed.\n",
            "trl 0.9.6 requires transformers>=4.31.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "shap 0.47.1 requires numba>=0.54, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "peft 0.14.0 requires transformers, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (2025.1.31)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "Successfully installed transformers-4.48.3\n",
            "Collecting datasets==3.2.0\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting optimum==1.24.0\n",
            "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (0.70.16)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.0)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (6.0.2)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum==1.24.0) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum==1.24.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets==3.2.0) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.2.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.2.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.2.0) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum==1.24.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum==1.24.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum==1.24.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum==1.24.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum==1.24.0) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.2.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.2.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.2.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.2.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum==1.24.0) (2.1.5)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets, optimum\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.12.0\n",
            "    Uninstalling fsspec-2024.12.0:\n",
            "      Successfully uninstalled fsspec-2024.12.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 3.4.1\n",
            "    Uninstalling datasets-3.4.1:\n",
            "      Successfully uninstalled datasets-3.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 fsspec-2024.9.0 optimum-1.24.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Collecting json-repair==0.29.1\n",
            "  Downloading json_repair-0.29.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting faker==35.2.0\n",
            "  Downloading Faker-35.2.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from faker==35.2.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from faker==35.2.0) (4.13.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading json_repair-0.29.1-py3-none-any.whl (15 kB)\n",
            "Downloading Faker-35.2.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: json-repair, faker\n",
            "Successfully installed faker-35.2.0 json-repair-0.29.1\n",
            "Collecting vllm==0.7.2\n",
            "  Downloading vllm-0.7.2-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Downloading vllm-0.7.2-cp38-abi3-manylinux1_x86_64.whl (264.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.3/264.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vllm\n",
            "Successfully installed vllm-0.7.2\n",
            "Collecting blake3\n",
            "  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting msgspec\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blake3, msgspec\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vllm 0.7.2 requires compressed-tensors==0.9.1, which is not installed.\n",
            "vllm 0.7.2 requires depyf==0.18.0, which is not installed.\n",
            "vllm 0.7.2 requires gguf==0.10.0, which is not installed.\n",
            "vllm 0.7.2 requires lark==1.2.2, which is not installed.\n",
            "vllm 0.7.2 requires lm-format-enforcer<0.11,>=0.10.9, which is not installed.\n",
            "vllm 0.7.2 requires mistral_common[opencv]>=1.5.0, which is not installed.\n",
            "vllm 0.7.2 requires outlines==0.1.11, which is not installed.\n",
            "vllm 0.7.2 requires partial-json-parser, which is not installed.\n",
            "vllm 0.7.2 requires prometheus-fastapi-instrumentator>=7.0.0, which is not installed.\n",
            "vllm 0.7.2 requires ray[default]>=2.9, which is not installed.\n",
            "vllm 0.7.2 requires xformers==0.0.28.post3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "vllm 0.7.2 requires xgrammar>=0.1.6; platform_machine == \"x86_64\", which is not installed.\n",
            "vllm 0.7.2 requires torch==2.5.1, but you have torch 2.6.0+cu124 which is incompatible.\n",
            "vllm 0.7.2 requires torchaudio==2.5.1, but you have torchaudio 2.6.0+cu124 which is incompatible.\n",
            "vllm 0.7.2 requires torchvision==0.20.1, but you have torchvision 0.21.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blake3-1.0.4 msgspec-0.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y numpy transformers vllm tensorflow numba\n",
        "!pip install numpy==1.26.4\n",
        "!pip install transformers==4.48.3\n",
        "!pip install datasets==3.2.0 optimum==1.24.0\n",
        "!pip install wandb json-repair==0.29.1 faker==35.2.0\n",
        "!pip install vllm==0.7.2 --no-deps\n",
        "!pip install blake3 msgspec\n",
        "!pip install uvloop partial_json_parser gguf xformers pyngrok locust"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip show uvloop partial-json-parser gguf xformers pyngrok vllm"
      ],
      "metadata": {
        "id": "H2A_lUEP_mUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHYb9GBA_VFD"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata  # Import Colab Secrets\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Load Gemini API key securely from Colab Secrets\n",
        "GOOGLE_API_KEY = userdata.get('gemini')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize the model\n",
        "gemini_model = genai.GenerativeModel('gemini-2.0-flash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WRE93KruQE_r",
        "outputId": "e0df2a6d-e6d9-4b5e-aa92-2e308d3117d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamdmandor2003\u001b[0m (\u001b[33mmohamdmandor2003-own\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `test`\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=userdata.get('wandb'))\n",
        "hf_token = userdata.get('huggingface')\n",
        "!huggingface-cli login --token {hf_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo7effJm98gn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "import json_repair\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "data_dir = \"/gdrive/MyDrive/LLM-Finetunning\"\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ijzl7zSXLS6"
      },
      "source": [
        "#Before Finetuning\n",
        "##**Structured info extraction Demo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsWVhp-2V2Nl"
      },
      "outputs": [],
      "source": [
        "story = \"\"\"\n",
        "ذكرت مجلة فوربس أن العائلة تلعب دورا محوريا في تشكيل علاقة الأفراد بالمال،\n",
        " حيث تتأثر هذه العلاقة بأنماط السلوك المالي المتوارثة عبر الأجيال.\n",
        "\n",
        "التقرير الذي يستند إلى أبحاث الأستاذ الجامعي شاين إنيت حول\n",
        "الرفاه المالي يوضح أن لكل شخص \"شخصية مالية\" تتحدد وفقا لطريقة\n",
        " تفاعله مع المال، والتي تتأثر بشكل مباشر بتربية الأسرة وتجارب الطفولة.\n",
        "\n",
        " الأبعاد الثلاثة للعلاقة بالمال\n",
        "بحسب الدراسة، هناك ثلاثة أبعاد رئيسية تشكّل علاقتنا بالمال:\n",
        "\n",
        "الاكتساب (A): يميل الأفراد الذين ينتمون لهذا\n",
        " البعد إلى اعتبار المال سلعة قابلة للجمع، حيث يرون\n",
        "في تحقيق الثروة هدفا بحد ذاته. والجانب السلبي لهذا\n",
        " النمط هو إمكانية التحول إلى هوس بالثروة أو العكس،\n",
        " أي رفض تام لاكتساب المال باعتباره مصدرا للفساد.\n",
        "\n",
        "الاستخدام (U): يرى هؤلاء الأشخاص المال أداة للتمتع بالحياة، حيث يربطون قيمته بقدرته على توفير\n",
        "المتعة والراحة. ومع ذلك، قد يصبح\n",
        "البعض مدمنا على الإنفاق، في حين يتجه آخرون إلى التقشف المفرط خوفا من المستقبل.\n",
        "\n",
        "الإدارة (M): أصحاب هذا النمط يعتبرون المال مسؤولية تتطلب التخطيط الدقيق. لكن في بعض الحالات،\n",
        " قد يتحول الأمر إلى هوس مفرط بإدارة الإنفاق، مما يؤثر سلبا على العلاقات الشخصية.\n",
        "\n",
        " كيف تؤثر العائلة على علاقتنا بالمال؟\n",
        "يشير التقرير إلى أن التجارب الأسرية تلعب دورا رئيسيا في تحديد\n",
        " \"الشخصية المالية\" لكل فرد، على سبيل المثال، إذا كان أحد الوالدين يعتمد على المال\n",
        "كمكافأة للسلوك الجيد، فقد يتبنى الطفل لاحقا النمط نفسه في حياته البالغة.\n",
        "\n",
        "لتحليل هذه التأثيرات بشكل دقيق، طورت رابطة العلاج المالي\n",
        "(Financial Therapy Association) أداة تسمى مخطط الجينوم المالي (Money Genogram)،\n",
        "وهو نموذج يُستخدم لتحديد الأنماط المالية داخل العائلة.\n",
        "\n",
        "تتضمن هذه الأداة:\n",
        "\n",
        "رسم شجرة عائلية.\n",
        "تصنيف أفراد العائلة وفقا للأبعاد الثلاثة للعلاقة بالمال (A ،U ،M).\n",
        "تحديد ما إذا كان السلوك المالي لكل فرد صحيا (+) أو غير صحي (-).\n",
        "على سبيل المثال، إذا نشأ شخص في عائلة\n",
        "اعتادت على الإنفاق المفرط، فقد يكون لديه ميل قوي إلى اتباع النمط نفسه،\n",
        " أو العكس تماما، حيث يصبح مقتصدا بشكل مبالغ فيه كرد فعل نفسي.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA0tu87C7yGd"
      },
      "outputs": [],
      "source": [
        "# story = \"\"\"\n",
        "# قرر المجلس القومي للأجور في مصر، زيادة الحد الأدنى لأجر العاملين بالقطاع الخاص إلى 7 آلاف جنيه شهريًا مقابل 6 آلاف جنيه، على أن يتم تطبيق الزيادة اعتبارًا من 1 مارس 2025.\n",
        "# كما قرر المجلس أن يكون الحد الأدنى لقيمة العلاوة الدورية للعاملين بالقطاع الخاص 250 جنيهًا شهريًا، ولأول مرة يقرر المجلس القومي للأجور وضع حد أدنى للأجر للعمل المؤقت \"جزء من الوقت\"، بحيث لا يقل أجرهم عن 28 جنيهًا صافيًا في الساعة، وذلك وفقًا لتعريفهم الوارد في قانون العمل.\n",
        "# وقالت وزيرة التخطيط والتنمية الاقتصادية والتعاون الدولي، رانيا المشاط، إن رفع الحد الأدنى للأجور يأتي في إطار الحرص على الاستجابة للمستجدات الاقتصادية الراهنة، بما يعزز الاستقرار الاقتصادي والاجتماعي، مضيفة أن ذلك يتسق مع المعايير الدولية، حيث تؤكد منظمة العمل الدولية على ضرورة مراجعة الحد الأدنى للأجور على أساس دوري، لحماية القوة الشرائية للأسر، واستيعاب التغيرات الاقتصادية التدريجية.\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRK9ESw4XM-g"
      },
      "source": [
        "## Details Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP7RPmHrXOAh"
      },
      "outputs": [],
      "source": [
        "# {\n",
        "#  \"story_title\": \"\",\n",
        "#  \"story_keywords\": [\"kw1\", \"kw2\"],\n",
        "#  \"story_summary\": [\"....\", \",,,,\"],\n",
        "#  \"story_category\": \"\",\n",
        "#  \"Story_entities\": [{\n",
        "#     \"story_value\": \"القاهره\",\n",
        "#     \"story_type\": \"location\"\n",
        "# }]\n",
        "# }\n",
        "\n",
        "StoryCategory = Literal[\"entertainment\", \"politics\", \"art\", \"technology\", \"food\", \"travel\", \"econmy\", \"not_specified\", \"sports\", \"economy\", \"health\", \"science\"]\n",
        "EntityType = Literal[\"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"]\n",
        "\n",
        "\n",
        "class Entity(BaseModel):\n",
        "  entity_value: str = Field(..., description=\"The actual name or the value of the entity.\")\n",
        "  entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n",
        "\n",
        "\n",
        "class NewsDetails(BaseModel):\n",
        "    story_title: str = Field(..., min_length=5, max_length=300, description=\"A fully informative and SEO optimized title of the story.\")  # ... means it required to be filled\n",
        "    story_keywords: List[str] = Field(..., min_items=1,description=\"Relevant keywords associated with the story.\")# we cant use list cuz i can customize ite elements so i will use List\n",
        "    story_summary: List[str] = Field(..., min_items=1, max_items=5,description=\"Summarized key points about the story (1-5 points).\")\n",
        "    story_category: StoryCategory = Field(..., description=\"Category of the news story.\") # need to specify the categories of it ,we will use literal like above\n",
        "    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,description=\"List of identified entities in the story.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1abhigkrEVrN"
      },
      "outputs": [],
      "source": [
        "# we used in content  join method in array elements cuz if we used   \"\"\" \"\"\"  it will take the spaces too to LLM :(\n",
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\" : \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \" \",\n",
        "            \"Pydantic Details:\",\n",
        "            json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n",
        "            \"\",\n",
        "            \"## Story Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "collapsed": true,
        "id": "V_TXyU2rh1wE",
        "outputId": "f5fa1237-cf94-4fb0-dea9-c50a612ad5ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[\"\\\\u0627\\\\u0644\\\\u0633\\\\u0644\\\\u0627\\\\u0645 \\\\u0639\\\\u0644\\\\u064a\\\\u0643\\\\u0645 \"]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# json.dumps([\"السلام عليكم \"], ensure_ascii=False)\n",
        "json.dumps([\"السلام عليكم \"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Xw4MRGaoI0g_",
        "outputId": "1266d5b1-984e-4b1a-f7fe-adb0e04c5712"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'$defs': {'Entity': {'properties': {'entity_value': {'description': 'The actual name or the value of the entity.',\n",
              "     'title': 'Entity Value',\n",
              "     'type': 'string'},\n",
              "    'entity_type': {'description': 'The type of recognized entity.',\n",
              "     'enum': ['person-male',\n",
              "      'person-female',\n",
              "      'location',\n",
              "      'organization',\n",
              "      'event',\n",
              "      'time',\n",
              "      'quantity',\n",
              "      'money',\n",
              "      'product',\n",
              "      'law',\n",
              "      'disease',\n",
              "      'artifact',\n",
              "      'not_specified'],\n",
              "     'title': 'Entity Type',\n",
              "     'type': 'string'}},\n",
              "   'required': ['entity_value', 'entity_type'],\n",
              "   'title': 'Entity',\n",
              "   'type': 'object'}},\n",
              " 'properties': {'story_title': {'description': 'A fully informative and SEO optimized title of the story.',\n",
              "   'maxLength': 300,\n",
              "   'minLength': 5,\n",
              "   'title': 'Story Title',\n",
              "   'type': 'string'},\n",
              "  'story_keywords': {'description': 'Relevant keywords associated with the story.',\n",
              "   'items': {'type': 'string'},\n",
              "   'minItems': 1,\n",
              "   'title': 'Story Keywords',\n",
              "   'type': 'array'},\n",
              "  'story_summary': {'description': 'Summarized key points about the story (1-5 points).',\n",
              "   'items': {'type': 'string'},\n",
              "   'maxItems': 5,\n",
              "   'minItems': 1,\n",
              "   'title': 'Story Summary',\n",
              "   'type': 'array'},\n",
              "  'story_category': {'description': 'Category of the news story.',\n",
              "   'enum': ['entertainment',\n",
              "    'politics',\n",
              "    'art',\n",
              "    'technology',\n",
              "    'food',\n",
              "    'travel',\n",
              "    'econmy',\n",
              "    'not_specified',\n",
              "    'sports',\n",
              "    'economy',\n",
              "    'health',\n",
              "    'science'],\n",
              "   'title': 'Story Category',\n",
              "   'type': 'string'},\n",
              "  'story_entities': {'description': 'List of identified entities in the story.',\n",
              "   'items': {'$ref': '#/$defs/Entity'},\n",
              "   'maxItems': 10,\n",
              "   'minItems': 1,\n",
              "   'title': 'Story Entities',\n",
              "   'type': 'array'}},\n",
              " 'required': ['story_title',\n",
              "  'story_keywords',\n",
              "  'story_summary',\n",
              "  'story_category',\n",
              "  'story_entities'],\n",
              " 'title': 'NewsDetails',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "NewsDetails.model_json_schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uouHmlxo8Tsw"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPHxQLVVvwh7"
      },
      "outputs": [],
      "source": [
        "# {\n",
        "#     \"translated_title\": \"\",\n",
        "#     \"translated_content\":\"\"\n",
        "# }\n",
        "\n",
        "\n",
        "class TranslatedStory(BaseModel):\n",
        "    translated_title: str = Field(..., min_length=5, max_length=300, description=\"Suggested Translated title to the news story.\")\n",
        "    translated_content: str = Field(...,min_length=5, description=\"The translated content of the news story.\")\n",
        "\n",
        "\n",
        "\n",
        "targeted_lang = \"English\"\n",
        "translation_messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\\n\".join([\n",
        "                    \"You are a professional translator.\",\n",
        "                    \"You will be provided by an Arabic text.\",\n",
        "                    f\"You have to translate the text into {targeted_lang} language.\",\n",
        "                    \"Follow the provided Scheme to generate a JSON\",\n",
        "                    \"Do not generate any introduction or conclusion.\"\n",
        "                ])\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\\n\".join([\n",
        "                    \"## Story:\",\n",
        "                    story.strip(),\n",
        "                    \"\",\n",
        "\n",
        "\n",
        "                    \"## Pydantic Details:\",\n",
        "                    json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "                    \"\",\n",
        "\n",
        "                    \"## Targeted Language or Dialect:\",\n",
        "                    targeted_lang,\n",
        "                    \"\",\n",
        "\n",
        "\n",
        "                    \"## Translated Story:\",\n",
        "                    \"```json\"\n",
        "                ])\n",
        "            }\n",
        "        ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wGL4lxH8qrq"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5_-JhBkzXC3"
      },
      "source": [
        "##**Qween**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRRVf_KskKUZ"
      },
      "outputs": [],
      "source": [
        "# load pretrained model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",    # when you work with GPU\n",
        "    torch_dtype=torch_dtype,  # load LLM in GPU memory\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iaho5Oymr7X",
        "outputId": "0f64daa2-f238-4080-8e97-b771cd14b850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"story_title\": \"How Family Influences Financial Behavior\",\n",
            "  \"story_keywords\": [\n",
            "    \"family influence\",\n",
            "    \"financial behavior\",\n",
            "    \"moneymaking\",\n",
            "    \"money management\",\n",
            "    \"inheritance\"\n",
            "  ],\n",
            "  \"story_summary\": [\n",
            "    \"Family plays a crucial role in shaping individuals' financial relationships.\",\n",
            "    \"Individuals inherit certain financial behaviors from their families.\"\n",
            "  ],\n",
            "  \"story_category\": \"economics\",\n",
            "  \"story_entities\": [\n",
            "    {\n",
            "      \"entity_value\": \"Forbes Magazine\",\n",
            "      \"entity_type\": \"organization\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"Shain Enit\",\n",
            "      \"entity_type\": \"person-female\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"Financial Therapy Association\",\n",
            "      \"entity_type\": \"organization\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# assiging task to model using template(chat template) that model has trained on like the below\n",
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "\n",
        "# return tensors in pytorch (pt) format , tokenization is in CPU memory so we need to send them to GPU memory so model can access to them\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generate_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False,\n",
        "    top_k=None,\n",
        "    top_p=None,\n",
        "    temperature=None)\n",
        "\n",
        "# tokenize the outputs to words\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids , output_ids in zip(model_inputs.input_ids, generate_ids)]\n",
        "\n",
        "\n",
        "# tansform the ids to words , skip_special_tokens=True  means hide special tokens\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]  # return list of reponse ( i use onlt 1 text so it will output 1 response but for future useage)\n",
        "# displaying model results (models lacks are 1- no arabic  /  2- limited entities have been extracted)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXVRU7JWBE1O",
        "outputId": "10649b37-6757-493a-93b0-dd2938dd3c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"translated_title\": \"Forbes Magazine Reveals Family Plays a Central Role in Forming Individuals' Financial Relationships\",\n",
            "  \"translated_content\": \"According to Forbes magazine, family plays a crucial role in shaping individuals' financial relationships, as these relationships are influenced by inherited behavioral patterns across generations.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generate_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False,\n",
        "    top_k=None,\n",
        "    top_p=None,\n",
        "    temperature=None)\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids , output_ids in zip(model_inputs.input_ids, generate_ids)]\n",
        "\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]  # return list of reponse ( i use onlt 1 text so it will output 1 response but for future useage)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ab-JL7fzceP"
      },
      "source": [
        "##**Gemini**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "qyCdMwI9jaCx",
        "outputId": "e4eda951-1a8f-468c-b792-bc98beced17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"story_title\": \"كيف تشكل العائلة علاقتك بالمال: الأبعاد الثلاثة وتأثير التجارب الأسرية\",\n",
            "  \"story_keywords\": [\n",
            "    \"العلاقة بالمال\",\n",
            "    \"الشخصية المالية\",\n",
            "    \"التجارب الأسرية\",\n",
            "    \"الأبعاد المالية\",\n",
            "    \"مخطط الجينوم المالي\"\n",
            "  ],\n",
            "  \"story_summary\": [\n",
            "    \"العائلة تلعب دورا محوريا في تشكيل علاقة الأفراد بالمال.\",\n",
            "    \"لكل شخص 'شخصية مالية' تتحدد وفقا لطريقة تفاعله مع المال.\",\n",
            "    \"هناك ثلاثة أبعاد رئيسية تشكل علاقتنا بالمال: الاكتساب، الاستخدام، والإدارة.\",\n",
            "    \"التجارب الأسرية تحدد 'الشخصية المالية' لكل فرد.\",\n",
            "    \"أداة 'مخطط الجينوم المالي' تستخدم لتحديد الأنماط المالية داخل العائلة.\"\n",
            "  ],\n",
            "  \"story_category\": \"economy\",\n",
            "  \"story_entities\": [\n",
            "    {\n",
            "      \"entity_value\": \"فوربس\",\n",
            "      \"entity_type\": \"organization\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"شاين إنيت\",\n",
            "      \"entity_type\": \"person-male\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"رابطة العلاج المالي\",\n",
            "      \"entity_type\": \"organization\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"مخطط الجينوم المالي\",\n",
            "      \"entity_type\": \"product\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Combine the messages into a single prompt for Gemini\n",
        "system_content = details_extraction_messages[0][\"content\"]\n",
        "user_content = details_extraction_messages[1][\"content\"]\n",
        "\n",
        "# Format the combined prompt for Gemini\n",
        "gemini_prompt = f\"{system_content}\\n\\n{user_content}\"\n",
        "\n",
        "# Call Gemini API with the formatted prompt\n",
        "gemini_extraction_response = gemini_model.generate_content(gemini_prompt)\n",
        "gemini_extraction_output = gemini_extraction_response.text\n",
        "\n",
        "\n",
        "try:\n",
        "    extracted_details_json = parse_json(gemini_extraction_output)\n",
        "    if extracted_details_json:\n",
        "        print(json.dumps(extracted_details_json, ensure_ascii=False, indent=2))\n",
        "    else:\n",
        "        print(\"Could not parse JSON from Gemini output. Raw output:\")\n",
        "        print(gemini_extraction_output)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"JSONDecodeError: {e}\")\n",
        "    print(\"Raw Gemini output:\")\n",
        "    print(gemini_extraction_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "jK1LHg93kcV6",
        "outputId": "32a95240-316b-47fa-d24f-df7d4f06625b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"translated_title\": \"How Family Shapes Your Relationship with Money: Forbes Report\", \"translated_content\": \"Forbes magazine reported that family plays a pivotal role in shaping individuals' relationship with money, as this relationship is influenced by patterns of financial behavior inherited across generations.\\n\\nThe report, based on research by Professor Shane Enete on financial well-being, explains that each person has a \\\"financial personality\\\" determined by their way of interacting with money, which is directly influenced by family upbringing and childhood experiences.\\n\\nThe Three Dimensions of the Relationship with Money\\nAccording to the study, there are three main dimensions that shape our relationship with money:\\n\\nAcquisition (A): Individuals belonging to this dimension tend to view money as a commodity to be collected, seeing wealth creation as a goal in itself. The downside of this pattern is the potential to turn into an obsession with wealth or, conversely, a complete rejection of acquiring money as a source of corruption.\\n\\nUsage (U): These people see money as a tool to enjoy life, linking its value to its ability to provide pleasure and comfort. However, some may become addicted to spending, while others tend towards excessive austerity for fear of the future.\\n\\nManagement (M): People with this pattern consider money a responsibility that requires careful planning. But in some cases, this may turn into an excessive obsession with managing spending, which negatively affects personal relationships.\\n\\nHow Does Family Affect Our Relationship with Money?\\nThe report indicates that family experiences play a key role in determining each individual's \\\"financial personality.\\\" For example, if one parent relies on money as a reward for good behavior, the child may later adopt the same pattern in their adult life.\\n\\nTo analyze these effects accurately, the Financial Therapy Association has developed a tool called the Money Genogram, a model used to identify financial patterns within the family.\\n\\nThis tool includes:\\n\\nDrawing a family tree.\\nClassifying family members according to the three dimensions of the relationship with money (A, U, M).\\nIdentifying whether each individual's financial behavior is healthy (+) or unhealthy (-).\\nFor example, if a person grew up in a family that used to overspend, they may have a strong tendency to follow the same pattern, or vice versa, becoming excessively frugal as a psychological reaction.\"}\n"
          ]
        }
      ],
      "source": [
        "system_content = translation_messages[0][\"content\"]\n",
        "user_content = translation_messages[1][\"content\"]\n",
        "gemini_prompt = f\"{system_content}\\n\\n{user_content}\"\n",
        "\n",
        "gemini_response = gemini_model.generate_content(gemini_prompt)\n",
        "gemini_output = gemini_response.text\n",
        "\n",
        "try:\n",
        "    translated_json = parse_json(gemini_output)\n",
        "    if translated_json:\n",
        "        print(json.dumps(translated_json, ensure_ascii=False))\n",
        "    else:\n",
        "        print(\"Could not parse JSON from Gemini output. Raw output:\")\n",
        "        print(gemini_output)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"JSONDecodeError: {e}\")\n",
        "    print(\"Raw Gemini output:\")\n",
        "    print(gemini_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z43W1hnuI3fX",
        "outputId": "797589bb-e7e5-4c32-cc70-2c5dbc257dba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'translated_title': 'How Family Shapes Your Relationship with Money: Forbes Report',\n",
              " 'translated_content': 'Forbes magazine reported that family plays a pivotal role in shaping individuals\\' relationship with money, as this relationship is influenced by patterns of financial behavior inherited across generations.\\n\\nThe report, based on research by Professor Shane Enete on financial well-being, explains that each person has a \"financial personality\" determined by their way of interacting with money, which is directly influenced by family upbringing and childhood experiences.\\n\\nThe Three Dimensions of the Relationship with Money\\nAccording to the study, there are three main dimensions that shape our relationship with money:\\n\\nAcquisition (A): Individuals belonging to this dimension tend to view money as a commodity to be collected, seeing wealth creation as a goal in itself. The downside of this pattern is the potential to turn into an obsession with wealth or, conversely, a complete rejection of acquiring money as a source of corruption.\\n\\nUsage (U): These people see money as a tool to enjoy life, linking its value to its ability to provide pleasure and comfort. However, some may become addicted to spending, while others tend towards excessive austerity for fear of the future.\\n\\nManagement (M): People with this pattern consider money a responsibility that requires careful planning. But in some cases, this may turn into an excessive obsession with managing spending, which negatively affects personal relationships.\\n\\nHow Does Family Affect Our Relationship with Money?\\nThe report indicates that family experiences play a key role in determining each individual\\'s \"financial personality.\" For example, if one parent relies on money as a reward for good behavior, the child may later adopt the same pattern in their adult life.\\n\\nTo analyze these effects accurately, the Financial Therapy Association has developed a tool called the Money Genogram, a model used to identify financial patterns within the family.\\n\\nThis tool includes:\\n\\nDrawing a family tree.\\nClassifying family members according to the three dimensions of the relationship with money (A, U, M).\\nIdentifying whether each individual\\'s financial behavior is healthy (+) or unhealthy (-).\\nFor example, if a person grew up in a family that used to overspend, they may have a strong tendency to follow the same pattern, or vice versa, becoming excessively frugal as a psychological reaction.'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parse_json(gemini_output) # if we not use this lib it will raise an error so we can tuse the reponse of the model in json format never"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHbHg9jvIwSE",
        "outputId": "6c3578aa-53a7-4c24-a632-f62bab4c7393"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(gemini_output) # cuz its str we need to do parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov866N0Q9eSO"
      },
      "source": [
        "#**Knowledge Distillation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQL2lr2WAt2T",
        "outputId": "4960f87d-c3d1-4bd5-a120-43610078cf03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw Data:2400\n"
          ]
        }
      ],
      "source": [
        "raw_dat_path = join(data_dir, \"datasets\", \"news-sample.jsonl\")\n",
        "raw_data = []\n",
        "\n",
        "for line in open(raw_dat_path, \"r\"):\n",
        "    if line.strip==\"\":\n",
        "        continue\n",
        "    raw_data.append(json.loads(line))\n",
        "\n",
        "\n",
        "random.Random(101).shuffle(raw_data)\n",
        "print(f\"Raw Data:{len(raw_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrKsrSg7mtba",
        "outputId": "127526ec-4043-4167-abef-1e492bab5b4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 975,\n",
              " 'title': 'ما تقدمه فلسطين للعالم.. معرض لآمال وآلام شعبها في باريس',\n",
              " 'description': 'يواصل المعهد العربي في باريس استقبال زواره في معرض “ما تقدمه فلسطين للعالم” لإطلاعهم على الإرث الثقافي والفني للفلسطينيين، من خلال أعمال فنية لآمالهم، وصور لواقعهم الأليم تحت الاحتلال.',\n",
              " 'content': 'يواصل المعهد العربي في باريس استقبال زواره في معرض ما تقدمه فلسطين للعالم لإطلاعهم على الإرث الثقافي والفني للفلسطينيين؛ من خلال أعمال فنية لآمالهم وصور لواقعهم الأليم تحت الاحتلال. \\n ويرى رئيس المعهد جاك لانغ -الذي أُعيد انتخابه قبل أيام للدورة الرابعة- ما يحدث في غزة حاليا جراء العدوان الإسرائيلي أنه كارثة. \\n والمعهد هو مركز ثقافي وواجهة دبلوماسية يديرها لانغ منذ 2013 ويقع على ضفة نهر السين في باريس. \\n وأشار لانغ، الذي شغل سابقا منصب وزير الثقافة بفرنسا، إلى أن المعرض هو إهداء للشعب الفلسطيني، ومُدّد ليستقبل مزيدا من الزوار حتى 31 ديسمبركانون الأول الجاري. \\n ويضم المعرض، الذي افتُتح أواخر مايوأيار الماضي، حسب لانغ العديد من المعارض الفرعية عن فلسطين وعن غزة بالتحديد، من بينها معرض الصور اليومية عن الحياة في غزة. \\n كما يشتمل على معرض الصور الفوتوكرومية القائم على تلوين صور من فلسطين تعود للقرن الـ19. \\n ويعرض الفنان الفلسطيني محمد أبو سل عملا فريدا بعنوان مترو غزة، وهو عبارة عن عمل تركيبي متعدد الوسائط، لاقى إعجابا من الزوار. \\n ويحضر الشاعر الفلسطيني الراحل محمود درويش من خلال أشعاره في أروقة المعرض، إلى جانب الكاتب والشاعر الفرنسي الملقب بعاشق فلسطين جان جينيه. \\n ومن الأعمال التي لاقت إعجاب الزوار عمل فني بعنوان بيوت غزة 2008-2009 عرضت فيه صور منازل في القطاع تدمرت بفعل القصف الإسرائيلي على غزة في 2008 و2009. \\n وقال لانغ لوكالة الأناضول أريد من المعرض أن يظهر الغنى الثقافي والإبداع لدى الشعب الفلسطيني من جميع النواحي. \\n وأشار إلى أن اسم الشعب الفلسطيني اقترن بالحرب ومن النادر أن يُسلط الضوء على ثقافته وفنه. \\n ويضم المعرض -أيضا- المتحف الفلسطيني في المنفى وأسس بالتعاون مع المؤرخ، سفير فلسطين الأسبق لدى اليونسكو إلياس صنبر. \\n ويضم تحفا تُبرع بها للمتحف لمدة 5 سنوات بإشراف المعهد العربي، وقال لانغ تعليقا على هذا المتحف نأمل أن نتمكن يوما ما من عرض هذا المتحف في القدس. \\n وبيّن لانغ أن المعرض حظي باهتمام كبير من الفرنسيين لا سيما الشباب، قائلا أريد التعريف أكثر بفلسطين، ثمة كثير من المعلومات المغلوطة حيال القضية الفلسطينية. \\n وأضاف رئيس المعهد العربي دُمّر العديد من المنازل والأحياء في غزة المعروضة صورها في المعرض. \\n وختم بالقول أعتقد أن وجود هذه الصور والأعمال هنا أمر جيد من أجل إبقاء الأمل لدى فناني غزة. \\n وكان المعهد قد أصدر كتابا جماعيا قبل شهور قليلة بعنوان ما تقدمه فلسطين للعالم، وصدر باللغة الفرنسية ضمن السلسلة الدورية عربوراما بالاشتراك مع دار النشر الفرنسية سوي Seuil. \\n ضم الكتاب مجموعة من المقالات الصحفية والدراسات الفكرية والبحوث الاجتماعية والقصائد الشعرية والصور والخرائط التوضيحية والرسومات الفنية واللوحات التشكيلية، لنخبة متميزة من الصحفيين والمفكرين والكتاب والشعراء والفنانين والرسامين والمؤرخين والباحثين العرب والأوروبيين، التي تتغنى كلها بحب سيدة الأرض فلسطين، وتحاول أن ترسم لها بالحرف والريشة صورة تتراوح بين الحلم والواقع. \\n والكتاب القيّم أسهم فيه 50 مبدعا على غرار المثقف والكاتب إلياس صنبر، والكاتبة وسفيرة فلسطين السابقة لدى اليونسكو ليلى شهيد، والمفكر والكاتب والصحفي الفرنسي المعروف آلان غريش، والكاتب والشاعر عبد اللطيف اللعبي، والفنانة التشكيلية أصالة شوك صاحبة الغلاف المميز للكتاب. \\n يحاول الكتاب أن يحفر أركيولوجيا وتاريخيا وأنثروبولوجيا وأدبيا وفنيا في مسار وطن تحوّل إلى رمز، وفكرة تحولت إلى قضية، وقضية توحّدت بشعب، وشعب فاض عن الأرض وتاه في المنفى والشتات والكون، باحثا عن صورة واقعية مطابقة لوطنه الأم المسلوب والمحاصر، فوجد نفسه في رحلة بحثه عن حق وحلم ومفتاح العودة يفتح أبوابا جديدة، ويكتشف قارات فكر بكر، ويخلق أوطانا وليدة في روحه وقلبه المتنصل من المكان والزمان والانتماء، استحضارا لصرخة الشاعر الرمز محمود درويش كل قلوب الناس جنسيتي.. فلتسقطوا عني جواز السفر. \\n يقول الكاتب كريستوف عياد في مقدمة الكتاب في الوقت الذي تبدو فيه فلسطين مهجورة من الجميع، بدءا من الدول العربية، اخترنا العودة إليها بطبيعة الحال للتحدث عن شعبها المشتت بسبب التاريخ والحدود. وأردنا مسح أراضيها المقسمة بين غزة والضفة الغربية على أن تكون القدس مركزا لا يمكن تعقبه. هذه الأراضي التي ضمها الاحتلال الإسرائيلي وابتلعها جدار الفصل العنصري. بعد أن أصبحت رمزا للاستعمار في عالم يمر بعملية إنهاء الاستعمار في النصف الثاني من القرن الـ20، فإن فلسطين لا تنتمي إلى نفسها. إنها قضية ومصدر إلهام للعالم كله. الكوفية علم الثوار في العالم، والفلسطيني لم يعد مجرد جنسية من دون دولة، بل هو رمز الرفض والانصياع والمقاومة. \\n من جانبه أشار جاك لانغ رئيس معهد العالم العربي بباريس إلى أن المعهد أراد من خلال هذا الكتاب الجماعي تقديم صورة إيجابية عن فلسطين؛ لأنه كثيرا ما يختصر الشعب الفلسطيني في النضال وفي المعاناة والحصار، ولكن يُتغافل عن الجانب الإبداعي الثقافي لهذا الشعب الذي يقدم الأفكار الجديدة المتميزة والإبداعات المتفردة إلى العالم. \\n وأشار لانغ -في حديثه السابق للجزيرة نت- إلى أن المعهد أراد أن يقدم هذا الإرث الثقافي المتميز في أبهى صورة إلى القراء، خاصة وهو يحمل قيمة كبيرة للثقافة العالمية، ومن هنا جاء عنوان الكتاب ما تقدمه فلسطين للعالم كشكل من أشكال الاعتراف بإبداعات الشعب الفلسطيني، وإضافاته الكبيرة إلى الثقافة العربية والإنسانية. \\n وأضاف الوضعية المعقدة والظلم الذي يعيشه الشعب الفلسطيني ليس وليد اليوم، وإنما هو نتيجة تراكمات تاريخية طويلة ومنذ عقود. وما يبعث على الحيرة والأسف في الوقت نفسه أننا لاحظنا هذا النسيان الدولي الذي تتعرض له القضية الفلسطينية العادلة، حتى من بعض الدول العربية نفسها. ومن هنا جاء هذا الكتاب، حتى نذكّر بهذه القضية وما يعيشه الشعب الفلسطيني من ظلم ونسيان، ونعيدها إلى مركز الأحداث والاهتمام الدولي. \\n وأشار إلى أن هذا الكتاب يحمل رسالة إلى المجتمع الدولي عن ضرورة الاعتراف بالحقوق المشروعة للشعب الفلسطيني، وأولها الحق في قيام دولة فلسطينية مستقلة. وشدد على أن المجتمع الدولي منقسم اليوم بخصوص القضية الفلسطينية، ولذلك جاء الكتاب بوصفه نوعا من الموقف والصرخة في وجه هذه المواقف المشتتة، وفي وجه الظلم الذي يتعرض له هذا الشعب، وهي صرخة تدعو المجتمع الدولي لضرورة عدم نسيان هذا الشعب المثقف المبدع والمناضل بالأفكار الأصيلة.',\n",
              " 'url': 'https://www.aljazeera.net/culture/2023/12/26/%d9%85%d8%a7-%d9%82%d8%af%d9%85%d8%aa%d9%87-%d9%81%d9%84%d8%b3%d8%b7%d9%8a%d9%86-%d9%84%d9%84%d8%b9%d8%a7%d9%84%d9%85-%d9%85%d8%b9%d8%b1%d8%b6-%d9%84%d8%a2%d9%85%d8%a7%d9%84',\n",
              " 'image': 'https://www.aljazeera.net/wp-content/uploads/2023/12/image-1703601365.jpg?resize=1920%2C1080&quality=80',\n",
              " 'datePublished': '2023-12-26T17:23:41Z',\n",
              " 'dateModified': '2023-12-26T17:23:41Z',\n",
              " 'section': 'ثقافة'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data[0] #its object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "FhRTZowLLe_J",
        "outputId": "671e90e0-ac6d-40b4-f1e3-a664a34b33ac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'يواصل المعهد العربي في باريس استقبال زواره في معرض ما تقدمه فلسطين للعالم لإطلاعهم على الإرث الثقافي والفني للفلسطينيين؛ من خلال أعمال فنية لآمالهم وصور لواقعهم الأليم تحت الاحتلال. \\n ويرى رئيس المعهد جاك لانغ -الذي أُعيد انتخابه قبل أيام للدورة الرابعة- ما يحدث في غزة حاليا جراء العدوان الإسرائيلي أنه كارثة. \\n والمعهد هو مركز ثقافي وواجهة دبلوماسية يديرها لانغ منذ 2013 ويقع على ضفة نهر السين في باريس. \\n وأشار لانغ، الذي شغل سابقا منصب وزير الثقافة بفرنسا، إلى أن المعرض هو إهداء للشعب الفلسطيني، ومُدّد ليستقبل مزيدا من الزوار حتى 31 ديسمبركانون الأول الجاري. \\n ويضم المعرض، الذي افتُتح أواخر مايوأيار الماضي، حسب لانغ العديد من المعارض الفرعية عن فلسطين وعن غزة بالتحديد، من بينها معرض الصور اليومية عن الحياة في غزة. \\n كما يشتمل على معرض الصور الفوتوكرومية القائم على تلوين صور من فلسطين تعود للقرن الـ19. \\n ويعرض الفنان الفلسطيني محمد أبو سل عملا فريدا بعنوان مترو غزة، وهو عبارة عن عمل تركيبي متعدد الوسائط، لاقى إعجابا من الزوار. \\n ويحضر الشاعر الفلسطيني الراحل محمود درويش من خلال أشعاره في أروقة المعرض، إلى جانب الكاتب والشاعر الفرنسي الملقب بعاشق فلسطين جان جينيه. \\n ومن الأعمال التي لاقت إعجاب الزوار عمل فني بعنوان بيوت غزة 2008-2009 عرضت فيه صور منازل في القطاع تدمرت بفعل القصف الإسرائيلي على غزة في 2008 و2009. \\n وقال لانغ لوكالة الأناضول أريد من المعرض أن يظهر الغنى الثقافي والإبداع لدى الشعب الفلسطيني من جميع النواحي. \\n وأشار إلى أن اسم الشعب الفلسطيني اقترن بالحرب ومن النادر أن يُسلط الضوء على ثقافته وفنه. \\n ويضم المعرض -أيضا- المتحف الفلسطيني في المنفى وأسس بالتعاون مع المؤرخ، سفير فلسطين الأسبق لدى اليونسكو إلياس صنبر. \\n ويضم تحفا تُبرع بها للمتحف لمدة 5 سنوات بإشراف المعهد العربي، وقال لانغ تعليقا على هذا المتحف نأمل أن نتمكن يوما ما من عرض هذا المتحف في القدس. \\n وبيّن لانغ أن المعرض حظي باهتمام كبير من الفرنسيين لا سيما الشباب، قائلا أريد التعريف أكثر بفلسطين، ثمة كثير من المعلومات المغلوطة حيال القضية الفلسطينية. \\n وأضاف رئيس المعهد العربي دُمّر العديد من المنازل والأحياء في غزة المعروضة صورها في المعرض. \\n وختم بالقول أعتقد أن وجود هذه الصور والأعمال هنا أمر جيد من أجل إبقاء الأمل لدى فناني غزة. \\n وكان المعهد قد أصدر كتابا جماعيا قبل شهور قليلة بعنوان ما تقدمه فلسطين للعالم، وصدر باللغة الفرنسية ضمن السلسلة الدورية عربوراما بالاشتراك مع دار النشر الفرنسية سوي Seuil. \\n ضم الكتاب مجموعة من المقالات الصحفية والدراسات الفكرية والبحوث الاجتماعية والقصائد الشعرية والصور والخرائط التوضيحية والرسومات الفنية واللوحات التشكيلية، لنخبة متميزة من الصحفيين والمفكرين والكتاب والشعراء والفنانين والرسامين والمؤرخين والباحثين العرب والأوروبيين، التي تتغنى كلها بحب سيدة الأرض فلسطين، وتحاول أن ترسم لها بالحرف والريشة صورة تتراوح بين الحلم والواقع. \\n والكتاب القيّم أسهم فيه 50 مبدعا على غرار المثقف والكاتب إلياس صنبر، والكاتبة وسفيرة فلسطين السابقة لدى اليونسكو ليلى شهيد، والمفكر والكاتب والصحفي الفرنسي المعروف آلان غريش، والكاتب والشاعر عبد اللطيف اللعبي، والفنانة التشكيلية أصالة شوك صاحبة الغلاف المميز للكتاب. \\n يحاول الكتاب أن يحفر أركيولوجيا وتاريخيا وأنثروبولوجيا وأدبيا وفنيا في مسار وطن تحوّل إلى رمز، وفكرة تحولت إلى قضية، وقضية توحّدت بشعب، وشعب فاض عن الأرض وتاه في المنفى والشتات والكون، باحثا عن صورة واقعية مطابقة لوطنه الأم المسلوب والمحاصر، فوجد نفسه في رحلة بحثه عن حق وحلم ومفتاح العودة يفتح أبوابا جديدة، ويكتشف قارات فكر بكر، ويخلق أوطانا وليدة في روحه وقلبه المتنصل من المكان والزمان والانتماء، استحضارا لصرخة الشاعر الرمز محمود درويش كل قلوب الناس جنسيتي.. فلتسقطوا عني جواز السفر. \\n يقول الكاتب كريستوف عياد في مقدمة الكتاب في الوقت الذي تبدو فيه فلسطين مهجورة من الجميع، بدءا من الدول العربية، اخترنا العودة إليها بطبيعة الحال للتحدث عن شعبها المشتت بسبب التاريخ والحدود. وأردنا مسح أراضيها المقسمة بين غزة والضفة الغربية على أن تكون القدس مركزا لا يمكن تعقبه. هذه الأراضي التي ضمها الاحتلال الإسرائيلي وابتلعها جدار الفصل العنصري. بعد أن أصبحت رمزا للاستعمار في عالم يمر بعملية إنهاء الاستعمار في النصف الثاني من القرن الـ20، فإن فلسطين لا تنتمي إلى نفسها. إنها قضية ومصدر إلهام للعالم كله. الكوفية علم الثوار في العالم، والفلسطيني لم يعد مجرد جنسية من دون دولة، بل هو رمز الرفض والانصياع والمقاومة. \\n من جانبه أشار جاك لانغ رئيس معهد العالم العربي بباريس إلى أن المعهد أراد من خلال هذا الكتاب الجماعي تقديم صورة إيجابية عن فلسطين؛ لأنه كثيرا ما يختصر الشعب الفلسطيني في النضال وفي المعاناة والحصار، ولكن يُتغافل عن الجانب الإبداعي الثقافي لهذا الشعب الذي يقدم الأفكار الجديدة المتميزة والإبداعات المتفردة إلى العالم. \\n وأشار لانغ -في حديثه السابق للجزيرة نت- إلى أن المعهد أراد أن يقدم هذا الإرث الثقافي المتميز في أبهى صورة إلى القراء، خاصة وهو يحمل قيمة كبيرة للثقافة العالمية، ومن هنا جاء عنوان الكتاب ما تقدمه فلسطين للعالم كشكل من أشكال الاعتراف بإبداعات الشعب الفلسطيني، وإضافاته الكبيرة إلى الثقافة العربية والإنسانية. \\n وأضاف الوضعية المعقدة والظلم الذي يعيشه الشعب الفلسطيني ليس وليد اليوم، وإنما هو نتيجة تراكمات تاريخية طويلة ومنذ عقود. وما يبعث على الحيرة والأسف في الوقت نفسه أننا لاحظنا هذا النسيان الدولي الذي تتعرض له القضية الفلسطينية العادلة، حتى من بعض الدول العربية نفسها. ومن هنا جاء هذا الكتاب، حتى نذكّر بهذه القضية وما يعيشه الشعب الفلسطيني من ظلم ونسيان، ونعيدها إلى مركز الأحداث والاهتمام الدولي. \\n وأشار إلى أن هذا الكتاب يحمل رسالة إلى المجتمع الدولي عن ضرورة الاعتراف بالحقوق المشروعة للشعب الفلسطيني، وأولها الحق في قيام دولة فلسطينية مستقلة. وشدد على أن المجتمع الدولي منقسم اليوم بخصوص القضية الفلسطينية، ولذلك جاء الكتاب بوصفه نوعا من الموقف والصرخة في وجه هذه المواقف المشتتة، وفي وجه الظلم الذي يتعرض له هذا الشعب، وهي صرخة تدعو المجتمع الدولي لضرورة عدم نسيان هذا الشعب المثقف المبدع والمناضل بالأفكار الأصيلة.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data[0]['content'] #its object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "196499a676fa4d71a1f312e02a05bc46",
            "a429826c6e3b41039c2513101602ab2c",
            "f8343ca0c8604c27a75a823aa7326d20",
            "eee8dfb45af445f08d568e122c72fc25",
            "9f1c73a4077e40cbbc2dd074a714e726",
            "dcf69a747ed44917904e8a03821b3bb0",
            "7229157e2f2845f6bd99d79a602298c3",
            "7daebe4cb2ca4741b8040f37b28547d4",
            "8391347738f24d6c8f9f08396ab2963f",
            "d6ce49d8080a441ca3e58b939172457f",
            "da2c535889fa41eaa5bc7e47246a5e07"
          ]
        },
        "collapsed": true,
        "id": "Mwoc-Pz0kvFy",
        "outputId": "4e8ce808-3dd1-4bae-a218-336c81cd2d92"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "196499a676fa4d71a1f312e02a05bc46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2400 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 3: Processed 3 stories successfully\n",
            "Iteration 6: Processed 6 stories successfully\n",
            "Iteration 9: Processed 9 stories successfully\n",
            "Iteration 12: Processed 12 stories successfully\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-77f78a324b31>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Call Gemini API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgemini_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgemini_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# knowledge distillation for Details Extraction\n",
        "save_to = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "ix = 0\n",
        "\n",
        "for story in tqdm(raw_data):\n",
        "    # Create messages structure\n",
        "    extraction_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are an NLP data parser.\",\n",
        "                \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "                \"Generate the output in the same story language.\",\n",
        "                \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "                \"Extract details as mentioned in text.\",\n",
        "                \"Do not generate any introduction or conclusion.\"\n",
        "            ])\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"## Story:\",\n",
        "                story['content'].strip(),\n",
        "                \"\",\n",
        "                \"Pydantic Details:\",\n",
        "                json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n",
        "                \"\",\n",
        "                \"## Story Details:\",\n",
        "                \"```json\"\n",
        "            ])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Combine for Gemini\n",
        "    system_content = extraction_messages[0][\"content\"]\n",
        "    user_content = extraction_messages[1][\"content\"]\n",
        "    gemini_prompt = f\"{system_content}\\n\\n{user_content}\"\n",
        "\n",
        "    try:\n",
        "        # Call Gemini API\n",
        "        response = gemini_model.generate_content(gemini_prompt)\n",
        "\n",
        "        if not response.text:\n",
        "            continue\n",
        "\n",
        "        # Parse the response\n",
        "        llm_resp_dict = parse_json(response.text)\n",
        "\n",
        "        if not llm_resp_dict:\n",
        "            continue\n",
        "\n",
        "        # Save the successful response\n",
        "        with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "            dest.write(json.dumps({\n",
        "                \"id\": ix,\n",
        "                \"story\": story['content'].strip(),\n",
        "                \"task\": \"Extract the story details into json.\",\n",
        "                \"output_scheme\": json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n",
        "                \"response\": llm_resp_dict,\n",
        "            }, ensure_ascii=False, default=str) + \"\\n\")\n",
        "\n",
        "        ix += 1\n",
        "\n",
        "        # Log progress\n",
        "        if (ix % 3) == 0:\n",
        "            print(f\"Iteration {ix}: Processed {ix} stories successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing story {ix}: {str(e)}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVGYaLrIwKbr"
      },
      "outputs": [],
      "source": [
        "# Knowledge Distillation for Translation\n",
        "save_to = join(data_dir, \"datasets\", \"xsft.jsonl\")\n",
        "\n",
        "ix = 0\n",
        "for story in tqdm(raw_data):\n",
        "    for targeted_lang in [\"English\", \"French\"]:\n",
        "        # Create the Gemini prompt for translation\n",
        "        prompt_gemini_extraction = \"\\n\".join([\n",
        "            \"You are a professional translator\",\n",
        "            \"You will be provided by an Arabic text\",\n",
        "            \"You have to translate the text into the Targeted Language\",\n",
        "            f\"You have to translate the text into {targeted_lang} language\",\n",
        "            \"Follow the provided Scheme to generate a JSON\",\n",
        "            \"Do not generate any introduction or conclusion\",\n",
        "            \"\",\n",
        "            \"Pydantic Details:\",\n",
        "            json.dumps(TranslatedStory.model_json_schema(), ensure_ascii=False),\n",
        "            \"\",\n",
        "            \"Story:\",\n",
        "            story['content'].strip(),\n",
        "            \"\",\n",
        "            \"Story Details in JSON:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            # Call Gemini API\n",
        "            response = gemini_model.generate_content(prompt_gemini_extraction)\n",
        "\n",
        "            if not response.text:\n",
        "                continue\n",
        "\n",
        "            # Parse the response\n",
        "            llm_resp_dict = parse_json(response.text)\n",
        "\n",
        "            if not llm_resp_dict:\n",
        "                continue\n",
        "\n",
        "            # Save the successful response\n",
        "            with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "                dest.write(json.dumps({\n",
        "                    \"id\": ix,\n",
        "                    \"story\": story['content'].strip(),\n",
        "                    \"task\": f\"You have to translate the story content into {targeted_lang} associated with a title into a JSON.\",\n",
        "                    \"output_scheme\": json.dumps(TranslatedStory.model_json_schema(), ensure_ascii=False),\n",
        "                    \"response\": llm_resp_dict,\n",
        "                }, ensure_ascii=False, default=str) + \"\\n\")\n",
        "\n",
        "            ix += 1\n",
        "\n",
        "            # Gemini doesn't provide token usage in the free tier, so we'll just track iterations\n",
        "            if (ix % 3) == 0:\n",
        "                print(f\"Iteration {ix}: Processed {ix} stories successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing story {ix}: {str(e)}\")\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYk_PzRlTtJ6"
      },
      "source": [
        "# Fromat Finetuning Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7YPLgnPlysF",
        "outputId": "c43427da-d374-4520-9f83-65a5a3b104bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'story': 'ظلت أسعار المنتجين بالولايات المتحدة دون تغيير في سبتمبرأيلول الماضي مدفوعة بانخفاض تكاليف البنزين، مما يشير إلى تقدم نحو تضخم أقل حدة، وهو ما يدعم توقعات خفض مجلس الاحتياطي الاتحادي المركزي الأميركي أسعار الفائدة مجددا الشهر المقبل. \\n وقال مكتب إحصاءات العمل التابع لوزارة العمل -في تقرير صدر اليوم الجمعة- إن القراءة الثابتة لمؤشر أسعار المنتجين للطلب النهائي الشهر الماضي جاءت بعد زيادة غير معدلة بلغت 0.2 في أغسطسآب الماضي. وعلى أساس سنوي، ارتفع المؤشر بنسبة 1.8، وهو أقل تقدم منذ فبرايرشباط الماضي. \\n ويُظهر التقرير أن مؤشرا أقل تقلبا، يُستخدم لقياس التضخم باستثناء الغذاء والطاقة والتجارة، ارتفع بنسبة 0.1، مما يعادل أقل زيادة منذ مايوأيار 2023. في وقت ظهرت فيه البيانات الخاصة بالتضخم العام والقطاعات التي يعتمد عليها الاحتياطي الفدرالي لاتخاذ قراراته. \\n وقد استقرت تكاليف الرعاية الطبية وتكاليف الرعاية الخارجية بالمستشفيات، في حين ارتفعت أسعار تذاكر الطيران بشكل حاد. \\n توقع المتداولون أن يخفض الاحتياطي الفدرالي أسعار الفائدة ربع نقطة مئوية الشهر المقبل، بعد أن بدأ حملته لتخفيف السياسات الشهر الماضي بخفض 50 نقطة أساس. \\n ومع ذلك، قد تؤدي التقارير الأخيرة -عن زيادة الوظائف وضغوط الأسعار المستمرة- إلى تعديل هذه التوقعات. \\n وأظهر التقرير أن تكاليف الخدمات ارتفعت بنسبة 0.2، في حين قفزت أسعار المواد الغذائية بنسبة 1، وهو أعلى مستوى منذ فبرايرشباط الماضي. \\n وفي المقابل، انخفضت أسعار الطاقة بنسبة 2.7، وكذلك تكاليف السلع المُعالجة للطلب الوسيط بنسبة 0.8، بسبب التراجع الكبير في أسعار وقود الديزل.',\n",
              " 'output_scheme': '{\"$defs\": {\"Entity\": {\"properties\": {\"entity_value\": {\"description\": \"The actual name or value of the entity.\", \"title\": \"Entity Value\", \"type\": \"string\"}, \"entity_type\": {\"description\": \"The type of entity recognized.\", \"enum\": [\"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\", \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"], \"title\": \"Entity Type\", \"type\": \"string\"}}, \"required\": [\"entity_value\", \"entity_type\"], \"title\": \"Entity\", \"type\": \"object\"}}, \"properties\": {\"story_title\": {\"description\": \"A fully informative and SEO optimized title of the story.\", \"maxLength\": 300, \"minLength\": 5, \"title\": \"Story Title\", \"type\": \"string\"}, \"story_keywords\": {\"description\": \"Relevant keywords associated with the story.\", \"items\": {\"type\": \"string\"}, \"minItems\": 1, \"title\": \"Story Keywords\", \"type\": \"array\"}, \"story_summary\": {\"description\": \"Summarized key points about the story (1-5 points).\", \"items\": {\"type\": \"string\"}, \"maxItems\": 5, \"minItems\": 1, \"title\": \"Story Summary\", \"type\": \"array\"}, \"story_category\": {\"description\": \"Category of the news story.\", \"enum\": [\"politics\", \"sports\", \"art\", \"technology\", \"economy\", \"health\", \"entertainment\", \"science\", \"not_specified\"], \"title\": \"Story Category\", \"type\": \"string\"}, \"story_entities\": {\"description\": \"List of identified entities in the story.\", \"items\": {\"$ref\": \"#/$defs/Entity\"}, \"maxItems\": 10, \"minItems\": 1, \"title\": \"Story Entities\", \"type\": \"array\"}}, \"required\": [\"story_title\", \"story_keywords\", \"story_summary\", \"story_category\", \"story_entities\"], \"title\": \"NewsDetails\", \"type\": \"object\"}',\n",
              " 'task': 'Extrat the story details into a JSON.',\n",
              " 'response': {'story_title': 'استقرار أسعار المنتجين في الولايات المتحدة يشير إلى تضخم أقل حدة',\n",
              "  'story_keywords': ['أسعار المنتجين',\n",
              "   'التضخم',\n",
              "   'مجلس الاحتياطي الفدرالي',\n",
              "   'الولايات المتحدة',\n",
              "   'الاقتصاد'],\n",
              "  'story_summary': ['أسعار المنتجين في الولايات المتحدة لم تتغير في سبتمبر.',\n",
              "   'انخفاض تكاليف البنزين ساهم في استقرار الأسعار.',\n",
              "   'توقعات بخفض أسعار الفائدة من قبل الاحتياطي الفدرالي.',\n",
              "   'زيادة طفيفة في مؤشر أسعار المنتجين على أساس سنوي.',\n",
              "   'ارتفاع أسعار المواد الغذائية وتذاكر الطيران.'],\n",
              "  'story_category': 'economy',\n",
              "  'story_entities': [{'entity_value': 'الولايات المتحدة',\n",
              "    'entity_type': 'location'},\n",
              "   {'entity_value': 'مجلس الاحتياطي الفدرالي', 'entity_type': 'organization'},\n",
              "   {'entity_value': 'أسعار المنتجين', 'entity_type': 'product'},\n",
              "   {'entity_value': 'البنزين', 'entity_type': 'product'},\n",
              "   {'entity_value': 'أسعار الفائدة', 'entity_type': 'money'},\n",
              "   {'entity_value': 'التضخم', 'entity_type': 'disease'},\n",
              "   {'entity_value': 'تكاليف الرعاية الطبية', 'entity_type': 'money'},\n",
              "   {'entity_value': 'أسعار المواد الغذائية', 'entity_type': 'money'},\n",
              "   {'entity_value': 'أسعار تذاكر الطيران', 'entity_type': 'money'},\n",
              "   {'entity_value': 'أسعار الطاقة', 'entity_type': 'money'}]}}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sft_data_path = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "\n",
        "llm_finetunning_data = []\n",
        "\n",
        "for line in open(sft_data_path):\n",
        "    if line.strip() == \"\": # empty line\n",
        "        continue\n",
        "    # llm_finetunning_data.append(json.loads(line))\n",
        "    rec = json.loads(line.strip())\n",
        "    break\n",
        "\n",
        "rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NglATSTF_-ej",
        "outputId": "c3c4312e-5f8c-4897-c677-c8e348c2bf6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Finetuning Data: 2778\n"
          ]
        }
      ],
      "source": [
        "sft_data_path = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "llm_finetunning_data = []\n",
        "\n",
        "# Simplified system message for fine-tuning\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "\n",
        "for line in open(sft_data_path, encoding='utf-8'):\n",
        "    if not line.strip():  # Skip empty lines\n",
        "        continue\n",
        "\n",
        "    rec = json.loads(line.strip())\n",
        "\n",
        "    # Construct structured training example\n",
        "    instruction_parts = [\n",
        "        \"# Story:\",\n",
        "        rec[\"story\"],\n",
        "        \"\",\n",
        "        \"# Task:\",\n",
        "        rec[\"task\"],\n",
        "        \"\",\n",
        "        \"# Output Scheme:\",\n",
        "        rec[\"output_scheme\"],\n",
        "        \"\",\n",
        "        \"# Output JSON:\",\n",
        "        \"```json\"\n",
        "    ]\n",
        "\n",
        "    output_parts = [\n",
        "        \"```json\",\n",
        "        json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "        \"```\"\n",
        "    ]\n",
        "\n",
        "    llm_finetunning_data.append({\n",
        "        \"system\": system_message,\n",
        "        \"instruction\": \"\\n\".join(instruction_parts),\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"\\n\".join(output_parts),\n",
        "        \"history\": []\n",
        "    })\n",
        "\n",
        "# Shuffle data with fixed seed for reproducibility\n",
        "random.Random(101).shuffle(llm_finetunning_data)\n",
        "print(f\"LLM Finetuning Data: {len(llm_finetunning_data)}\")  # in tutorial = 2766"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrCfxBja35rf"
      },
      "outputs": [],
      "source": [
        "# i need to split data into train,test,validate\n",
        "\n",
        "train_sample_sz = 2700\n",
        "\n",
        "train_ds= llm_finetunning_data[:train_sample_sz] #all before 2700 till the end\n",
        "eval_ds= llm_finetunning_data[train_sample_sz:]# after 2700 till the end\n",
        "\n",
        "\n",
        "os.makedirs(join(data_dir, \"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "# w is for writing access\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xD5LrF265ti3",
        "outputId": "b3b2309a-27e5-47ff-9525-4ed9cb7d29aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/MyDrive/LLM-Finetunning/datasets/llamafactory-finetune-data/val.json'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMAyfmEsSMaQ"
      },
      "source": [
        "# Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWHbmqpUuLPw"
      },
      "outputs": [],
      "source": [
        "# # Configure LLaMA-Factory for the new datasets\n",
        "\n",
        "# # update /content/LLaMA-Factory/data/dataset_info.json and append\n",
        "# ```\n",
        "   \"news_finetune_train\": {\n",
        "        \"file_name\": \"/gdrive/MyDrive/LLM-Finetunning/datasets/llamafactory-finetune-data/train.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    },\n",
        "    \"news_finetune_val\": {\n",
        "        \"file_name\": \"/gdrive/MyDrive/LLM-Finetunning/datasets/llamafactory-finetune-data/val.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    }\n",
        "# ```\n",
        "\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/apwbkni9\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/c5tf0q90"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDB6rZUpCwRv"
      },
      "source": [
        "we need to write new file inside train_lora(optional but prefered)\n",
        "%% is magic command\n",
        "anthing we write in the following cell is written inside news_finetune.yaml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MXN83Y5DAFi",
        "outputId": "28976f63-434a-4bcb-bf73-374c4bcc830b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "# resume_from_checkpoint: /gdrive/MyDrive//LLM-Finetunning/models/checkpoint-1500\n",
        "output_dir: /gdrive/MyDrive/LLM-Finetunning/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: newsx-finetune-llamafactory\n",
        "\n",
        "# push_to_hub: true\n",
        "# export_hub_model_id: \"bakrianoo/news-analyzer\"\n",
        "# hub_private_repo: true\n",
        "# hub_strategy: checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EjnMzsxIlNa",
        "outputId": "fbc11300-06eb-4ef4-fdf9-800f19a5e393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 03-30 15:50:23 __init__.py:190] Automatically detected platform cuda.\n",
            "[INFO|2025-03-30 15:50:27] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,058 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,058 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,058 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,058 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,058 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,058 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,058 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2304] 2025-03-30 15:50:28,413 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-03-30 15:50:28,763 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-03-30 15:50:28,765 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,850 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,850 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,850 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,850 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,850 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,850 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-30 15:50:28,850 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2304] 2025-03-30 15:50:29,202 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-03-30 15:50:29] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-03-30 15:50:29] llamafactory.data.loader:143 >> Loading dataset /gdrive/MyDrive/LLM-Finetunning/datasets/llamafactory-finetune-data/train.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 2700 examples [00:01, 2336.52 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 2700/2700 [00:01<00:00, 2462.59 examples/s]\n",
            "[INFO|2025-03-30 15:50:32] llamafactory.data.loader:143 >> Loading dataset /gdrive/MyDrive/LLM-Finetunning/datasets/llamafactory-finetune-data/val.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 78 examples [00:00, 2525.38 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 78/78 [00:00<00:00, 190.43 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  38% 1014/2700 [00:31<00:24, 69.81 examples/s]Process ForkPoolWorker-39:\n",
            "Process ForkPoolWorker-40:\n",
            "Process ForkPoolWorker-38:\n",
            "Process ForkPoolWorker-37:\n",
            "Process ForkPoolWorker-36:\n",
            "Process ForkPoolWorker-35:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/queues.py\", line 367, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/queues.py\", line 367, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/queues.py\", line 367, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/queues.py\", line 368, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 219, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 433, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "          ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 398, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/queues.py\", line 367, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/queues.py\", line 367, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Running tokenizer on dataset (num_proc=16):  38% 1014/2700 [00:32<00:53, 31.27 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 704, in iflatmap_unordered\n",
            "    yield queue.get(timeout=0.05)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<string>\", line 2, in get\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/managers.py\", line 822, in _callmethod\n",
            "    kind, result = conn.recv()\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 253, in recv\n",
            "    buf = self._recv_bytes()\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 433, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "          ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 398, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 117, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 107, in run_exp\n",
            "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 69, in _training_function\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
            "    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 303, in get_dataset\n",
            "    dataset = _get_preprocessed_dataset(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 249, in _get_preprocessed_dataset\n",
            "    dataset = dataset.map(\n",
            "              ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 3165, in map\n",
            "    for rank, done, content in iflatmap_unordered(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 718, in iflatmap_unordered\n",
            "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 718, in <listcomp>\n",
            "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 768, in get\n",
            "    self.wait(timeout)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 765, in wait\n",
            "    self._event.wait(timeout)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 629, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 331, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awlfADekXVEv"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6WUHSLkSyYt"
      },
      "source": [
        "## New Finetuned Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "95ed09cc5c104cb7ba77a12a9563c230",
            "0a8f2cca2e12459f9fda5612e63752ff",
            "27815451e7394f2a881b28d2cd02f2b6",
            "b3d19e4074e84a568eb1615c79ebe15c",
            "94da70dd7c1a4b4fb2a937091398e24a",
            "982e7af5e0684f9189e359faa1537a42",
            "942e45351f8a4a45b8c1c094d83ab2a2",
            "41bf4f8f3fee4d30b4e8dc652151c82e",
            "cb0f0b8d3f874694a68a5dfe894844ce",
            "bd364054ad994f989582f76cbc126412",
            "9d6b672eb1c3490bb989cf24df3182c8",
            "1116fb6241db4b2182b3690f7361877b",
            "c38c7244a7864c14907c9c2d888b27fa",
            "ae6cd78d1c57464aade88be1b70f4441",
            "1c80f0b368ea4718af70110ea36d6b80",
            "037cab93391f4b34b2d66c4b6fd70468",
            "f7ce935c0041408f8442f9970380e108",
            "dcbadfd14e6d481e91539946417e581c",
            "923aa191171d4b79b717a1bf8f6f5020",
            "d9ea8ff7a0594b66858164080f163a5c",
            "c0443fc01e664fc983ae11c96f06b46a",
            "e9b7d9e50f3b4e9d8ff93e88ccc93bb6",
            "37b270195dd14976af1328b082466743",
            "c5608ea88d584555bfda4b5f02d75899",
            "0178074f03eb43d095997f04fccb742f",
            "fa0fbd4f6e6c4696967ed48d94cfdda8",
            "56950b2c2b5042378f8682b883ab34d1",
            "ff22ebe5534d408ebc80046d3f3a9604",
            "d6e94dd9012745eabc0d713c34d9c660",
            "b69bdc55085f4bbcbbc10f5657a67919",
            "73ccd1070121494cbe3a5165959061f3",
            "babdc45323674142aea58f5bcfce51ea",
            "b44a271c0606439dbee0ef9481089c4f",
            "7ed19de0e4484fcc811af8f238346d55",
            "423fbd8e3bf942339803c81f0bbc48a3",
            "7a235530e4d14c88b5ddd59ff6962b27",
            "71aff970b5614fcc960ed2a626205b59",
            "913d3e8d27cd4cbea3d8c1827d1d5b93",
            "57e962feaf514798a97ccd5ede63a5f1",
            "0b77e4b586044aca836c58e9bad9bbac",
            "f5abffac868f40648c70d310ddc832a5",
            "2d9dbf1b880647cc83ac8831843cab62",
            "f0aaf2f49f8c4296bde0e81b4d7ce7ad",
            "3ad41c46102a4b56a10b2c1157844848",
            "c8ea807f74484c92a0c979ddf470605f",
            "9e0fa50fd2104a93a2ca9f86f5045e43",
            "56266085a47748e2a48e622e28ac7d2c",
            "7a5b8cbd5cf344e9b7e3760203d0db9b",
            "6087082f014b474c9f27b75825e9b447",
            "61029ab2ef6e4a149bb8d4cbc67ba02a",
            "fe28717fb023455b90ffcbafc241a2ae",
            "19a95be6366d40ca92d42768947e7725",
            "a099c34d8b7d4e47adfc38ab2b299383",
            "b3be8cbc9fe94b15949b1743f1e9e855",
            "3653c55f7f114385858ac6e809107768",
            "03bbcada456c4f5d94f1766c0853475f",
            "91b86e78252b443b9d874723a55552ff",
            "fefcb96a851c4ebeaacb2a1238518b18",
            "28e654f30dbf425185f010f829f78ff6",
            "749c927e8fe749f8b72e6a89b79abab1",
            "b8c9c010d10d49d2963ebba06d66cf76",
            "6c3414d2ef664a6bad5d889be17721ec",
            "8f70917bb9194311a25b98bc3002b65f",
            "90ac7cd7b35c43a19312f878d9193759",
            "00d584d627a8462cb0e60c3895440040",
            "c0c6759e9f74405bb235a8e8ba3c7eb7",
            "3c3f9feb9a844024824a3ed71ab58694",
            "3c1a265e316a485d814b8837cd064dcd",
            "bd199efff79c46a9a93be41a52aa656e",
            "a529fbac3a5741be84223f8cfb643833",
            "3e1b008e0ce441dc9809133141cb05b5",
            "f8649e903b174da88a6c9eefaf88c307",
            "016fb16a7c5a4076b7e371f48afd2455",
            "87e16ce01eb3468897951ce6aa1cdb3c",
            "af02ee05f7c5408ea4dfaac442548790",
            "3da0666d8ebe49058c134fcea672f379",
            "4932532b79ba4defb2d1d57e1a6c7a3e"
          ]
        },
        "id": "Hw46VNViXPyG",
        "outputId": "bfd821b6-626e-4e81-8742-e773b8156a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95ed09cc5c104cb7ba77a12a9563c230"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1116fb6241db4b2182b3690f7361877b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37b270195dd14976af1328b082466743"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ed19de0e4484fcc811af8f238346d55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8ea807f74484c92a0c979ddf470605f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03bbcada456c4f5d94f1766c0853475f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c3f9feb9a844024824a3ed71ab58694"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA6w4528vxrq",
        "outputId": "ce42a7b0-c68c-4cd5-d967-91ad651cec9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 1536)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=1536, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (k_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=1536, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=256, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (v_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=1536, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=256, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (o_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=1536, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=1536, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=8960, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (up_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=1536, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=8960, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (down_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=8960, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "finetuned_model_id = \"/gdrive/MyDrive/LLM-Finetunning/models\"\n",
        "model.load_adapter(finetuned_model_id)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets Test Finetuned Qween with Extraction task and see if it will Generate it in Arabic**"
      ],
      "metadata": {
        "id": "jwhtw7am-kWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assiging task to model using template(chat template) that model has trained on like the below\n",
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "\n",
        "# return tensors in pytorch (pt) format , tokenization is in CPU memory so we need to send them to GPU memory so model can access to them\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generate_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False,\n",
        "    top_k=None,\n",
        "    top_p=None,\n",
        "    temperature=None)\n",
        "\n",
        "# tokenize the outputs to words\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids , output_ids in zip(model_inputs.input_ids, generate_ids)]\n",
        "\n",
        "\n",
        "# tansform the ids to words , skip_special_tokens=True  means hide special tokens\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]  # return list of reponse ( i use onlt 1 text so it will output 1 response but for future useage)\n",
        "# displaying model results (models lacks are 1- no arabic  /  2- limited entities have been extracted)\n",
        "parse_json(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Bm0SAUcXzH",
        "outputId": "08fc59a5-702b-4160-9277-89e92581e40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'story_title': 'تأثير العائلة على علاقة الأفراد بالمال',\n",
              " 'story_keywords': ['العائلة',\n",
              "  'ال金钱',\n",
              "  'السلوكيات المالية',\n",
              "  'الصحة المالية',\n",
              "  'تخطيط النفقات'],\n",
              " 'story_summary': ['العلاقة بين الأفراد والمال تعتمد على أنماط السلوك المالي.',\n",
              "  'الثلاثة أبعاد الرئيسية للعلاقة بالمال هي: الاكتساب، الاستخدام، والإدارة.',\n",
              "  'التجارب الأسرية تؤثر على شخصية每个人的财务管理.',\n",
              "  'تقرير عن العلاقة بين العائلة والمال يقدم نصائح عملية.'],\n",
              " 'story_category': 'economy',\n",
              " 'story_entities': [{'entity_value': 'فوربس', 'entity_type': 'organization'},\n",
              "  {'entity_value': 'شاين إنيت', 'entity_type': 'person-male'},\n",
              "  {'entity_value': 'رابطة العلاج المالي', 'entity_type': 'organization'},\n",
              "  {'entity_value': 'Money Genogram', 'entity_type': 'artifact'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Translation Taks it exceed and complete the full message not like before**"
      ],
      "metadata": {
        "id": "Iq9KHf4m-2v3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYT8TMqqXcP3",
        "outputId": "da9a6148-3307-400d-d96b-1c410d219bb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'translated_title': 'How Family Influences Financial Relationships',\n",
              " 'translated_content': 'Forbes magazine reported that the family plays a pivotal role in shaping individuals\\' relationship with money, as this relationship is influenced by inherited financial behaviors across generations.\\n\\nThe report, based on research by Professor Shane Everette on financial well-being, explains that each person has a \\'financial personality\\' determined by how they interact with money, which is directly affected by family upbringing and childhood experiences.\\n\\nThe three dimensions of our financial relationship according to the study include:\\n\\nAcquisition (A): Individuals belonging to this dimension tend to view money as a commodity that can be accumulated, seeing wealth accumulation as a goal in itself. The downside of this pattern is the potential for it to turn into an obsession with wealth or vice versa, meaning complete rejection of acquiring money as a means of corruption.\\n\\nUsage (U): These individuals see money as a tool for enjoying life, linking its value to their ability to provide enjoyment and comfort. However, some may become compulsive spenders while others gravitate towards excessive frugality fearing the future.\\n\\nManagement (M): Those with this mindset consider money a responsibility that requires careful planning. In some cases, it may turn into an obsessive preoccupation with managing spending, negatively affecting personal relationships.\\n\\nHow does family influence our relationship with money? The report indicates that family experiences play a crucial role in determining \"the financial personality\" of each individual, for example, if one parent relies on money as a reward for good behavior, the child may later adopt the same pattern in their adult life.\\n\\nTo analyze these influences accurately, the Financial Therapy Association developed a tool called the Money Genome Map (Money Genogram), used to identify financial patterns within families.\\n\\nThis tool includes:\\n\\nDrawing a family tree.\\nClassifying family members according to the three dimensions of financial relationship (A, U, M).\\nDetermining whether the financial behavior of each individual is healthy (+) or unhealthy (-). For instance, if someone grew up in a family accustomed to excessive spending, they may have a strong inclination to adopt the same pattern, or the opposite, where they become excessively stingy as a psychological reaction.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def generate_resp(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        translation_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "response = generate_resp(translation_messages)\n",
        "parse_json(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0zmfO1vaCn8"
      },
      "source": [
        "# Cost Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "77c974eb60e24d038b538ede01e8a10d",
            "64cc8430948f44ddb89a12f4ec2fc1a9",
            "f936f4c117cd42a18bb705fa85164c39",
            "fc3c72bd7dc2486b88d1a2e72e937bd1",
            "9b8bbc2772444e68b775ee8ce8d29582",
            "8d3ce2b42c2a4635ae95b1981bfbb8a4",
            "df3e428ca22643d79e7420a106b6315a",
            "253c8e33f63b4ba8a507fc3392761f6d",
            "f8e24e22227745bea0607d53bcadb26c",
            "fe8001f926834a40bf4039814fcd12e8",
            "e49240adb2fe45568210e762b4450404"
          ]
        },
        "id": "--HMKdMvaEaW",
        "outputId": "a8b391c2-7afc-4431-86af-bdd2b79b7835"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77c974eb60e24d038b538ede01e8a10d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Time: 861.590396 seconds\n",
            "Input Tokens: 2489\n",
            "Output Tokens: 13740\n",
            "Total Tokens: 16229\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "start_time = datetime.now()\n",
        "fake = Faker('ar')\n",
        "\n",
        "input_tokens = 0\n",
        "output_tokens = 0\n",
        "\n",
        "for i in tqdm(range(30)):\n",
        "    prompt = fake.text(max_nb_chars=random.randint(150, 200))\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = generate_resp(messages)\n",
        "\n",
        "    input_tokens += len(tokenizer.apply_chat_template(messages))\n",
        "    output_tokens += len(tokenizer.encode(response))\n",
        "\n",
        "total_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"Total Time: {total_time} seconds\")\n",
        "print(f\"Input Tokens: {input_tokens}\")\n",
        "print(f\"Output Tokens: {output_tokens}\")\n",
        "print(f\"Total Tokens: {input_tokens + output_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZZj07lqoA0e",
        "outputId": "fa80799a-c52b-412f-e6a7-0250cf93d034"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15.958188153310104"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "13740 /861  # 15 tokens/second"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtlR9nInoH6a"
      },
      "source": [
        "# vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "5WwdoXPYB_Xm"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "adapter_model_id = \"/gdrive/MyDrive/LLM-Finetunning/models\"\n",
        "\n",
        "!nohup vllm serve \"{base_model_id}\" --dtype=half --gpu-memory-utilization 0.4 --max_lora_rank 64 --enable-lora --lora-modules news-lora=\"{adapter_model_id}\" --max_model_len 2048 --port 8001 --enforce-eager > nohup.out 2> nohup.err &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzPiVlfuS23U",
        "outputId": "77c06086-391c-44c4-bde0-e4dc1d3cb79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       12908  0.0  0.0   7376  3364 ?        S    23:52   0:00 /bin/bash -c ps aux | grep vllm\n",
            "root       12910  0.0  0.0   6484  2112 ?        S    23:52   0:00 grep vllm\n"
          ]
        }
      ],
      "source": [
        "# !pkill vllm\n",
        "# !ps aux | grep vllm\n",
        "# !kill 34339\n",
        "# ngrok.kill()  # This will terminate all existing ngrok tunnels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "5Uf4h5TnqyP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d8e63a-8212-469e-cac6-3c792df557d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-02 23:43:35 __init__.py:190] Automatically detected platform cuda.\n",
            "INFO 04-02 23:43:37 api_server.py:840] vLLM API server version 0.7.2\n",
            "INFO 04-02 23:43:37 api_server.py:841] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-1.5B-Instruct', config='', host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='news-lora', path='/gdrive/MyDrive/LLM-Finetunning/models', base_model_name=None)], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-1.5B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.4, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7e3a93b28860>)\n",
            "INFO 04-02 23:43:37 api_server.py:206] Started engine process with PID 10414\n",
            "WARNING 04-02 23:43:40 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 04-02 23:43:46 __init__.py:190] Automatically detected platform cuda.\n",
            "WARNING 04-02 23:43:51 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 04-02 23:43:54 config.py:542] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
            "WARNING 04-02 23:43:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "WARNING 04-02 23:43:54 config.py:678] Async output processing is not supported on the current platform type cuda.\n",
            "INFO 04-02 23:44:02 config.py:542] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
            "WARNING 04-02 23:44:02 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "WARNING 04-02 23:44:02 config.py:678] Async output processing is not supported on the current platform type cuda.\n",
            "INFO 04-02 23:44:02 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
            "INFO 04-02 23:44:04 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 04-02 23:44:04 cuda.py:227] Using XFormers backend.\n",
            "INFO 04-02 23:44:05 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
            "INFO 04-02 23:44:06 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 04-02 23:44:07 weight_utils.py:297] No model.safetensors.index.json found in remote.\n",
            "INFO 04-02 23:44:25 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
            "INFO 04-02 23:44:25 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 04-02 23:44:31 worker.py:267] Memory profiling takes 5.55 seconds\r\n",
            "INFO 04-02 23:44:31 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.40) = 5.90GiB\r\n",
            "INFO 04-02 23:44:31 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 1.57GiB.\n",
            "INFO 04-02 23:44:31 executor_base.py:110] # CUDA blocks: 3676, # CPU blocks: 9362\n",
            "INFO 04-02 23:44:31 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 28.72x\n",
            "INFO 04-02 23:44:36 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 11.25 seconds\n",
            "INFO 04-02 23:44:37 api_server.py:756] Using supplied chat template:\r\n",
            "INFO 04-02 23:44:37 api_server.py:756] None\n",
            "INFO 04-02 23:44:43 serving_models.py:174] Loaded new LoRA adapter: name 'news-lora', path '/gdrive/MyDrive/LLM-Finetunning/models'\n",
            "INFO 04-02 23:44:43 launcher.py:21] Available routes are:\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /openapi.json, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /docs, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /redoc, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /health, Methods: GET\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /ping, Methods: GET, POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /tokenize, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /detokenize, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/models, Methods: GET\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /version, Methods: GET\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/completions, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/embeddings, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /pooling, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /score, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/score, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /rerank, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/rerank, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v2/rerank, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /invocations, Methods: POST\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.87s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.87s/it]\n",
            "\n",
            "INFO:     Started server process [10316]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
          ]
        }
      ],
      "source": [
        "!tail -n 100 nohup.out\n",
        "!tail -n 100 nohup.err"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_token = userdata.get('ngrok')\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "public_url = ngrok.connect(8001).public_url\n",
        "print(f\"vLLM API server available at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IAC0ypU7Ezq",
        "outputId": "9831d08f-0dd5-4acd-d74d-7b7a98420c00"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "vLLM API server available at: https://522c-34-124-187-102.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtWYU1tzq7jL"
      },
      "source": [
        "# inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "EIiYaS-vbDmU"
      },
      "outputs": [],
      "source": [
        "story = \"\"\"\n",
        "قرر المجلس القومي للأجور في مصر، زيادة الحد الأدنى لأجر العاملين بالقطاع الخاص إلى 7 آلاف جنيه شهريًا مقابل 6 آلاف جنيه، على أن يتم تطبيق الزيادة اعتبارًا من 1 مارس 2025.\n",
        "كما قرر المجلس أن يكون الحد الأدنى لقيمة العلاوة الدورية للعاملين بالقطاع الخاص 250 جنيهًا شهريًا، ولأول مرة يقرر المجلس القومي للأجور وضع حد أدنى للأجر للعمل المؤقت \"جزء من الوقت\"، بحيث لا يقل أجرهم عن 28 جنيهًا صافيًا في الساعة، وذلك وفقًا لتعريفهم الوارد في قانون العمل.\n",
        "وقالت وزيرة التخطيط والتنمية الاقتصادية والتعاون الدولي، رانيا المشاط، إن رفع الحد الأدنى للأجور يأتي في إطار الحرص على الاستجابة للمستجدات الاقتصادية الراهنة، بما يعزز الاستقرار الاقتصادي والاجتماعي، مضيفة أن ذلك يتسق مع المعايير الدولية، حيث تؤكد منظمة العمل الدولية على ضرورة مراجعة الحد الأدنى للأجور على أساس دوري، لحماية القوة الشرائية للأسر، واستيعاب التغيرات الاقتصادية التدريجية.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "HRVoI8ejrA6i",
        "outputId": "a8cca76b-6eb7-45c9-99c2-97dd6fbd0059"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nYou are a professional translator.\\nYou will be provided by an Arabic text.\\nYou have to translate the text into English language.\\nFollow the provided Scheme to generate a JSON\\nDo not generate any introduction or conclusion.<|im_end|>\\n<|im_start|>user\\n## Story:\\nقرر المجلس القومي للأجور في مصر، زيادة الحد الأدنى لأجر العاملين بالقطاع الخاص إلى 7 آلاف جنيه شهريًا مقابل 6 آلاف جنيه، على أن يتم تطبيق الزيادة اعتبارًا من 1 مارس 2025.\\nكما قرر المجلس أن يكون الحد الأدنى لقيمة العلاوة الدورية للعاملين بالقطاع الخاص 250 جنيهًا شهريًا، ولأول مرة يقرر المجلس القومي للأجور وضع حد أدنى للأجر للعمل المؤقت \"جزء من الوقت\"، بحيث لا يقل أجرهم عن 28 جنيهًا صافيًا في الساعة، وذلك وفقًا لتعريفهم الوارد في قانون العمل.\\nوقالت وزيرة التخطيط والتنمية الاقتصادية والتعاون الدولي، رانيا المشاط، إن رفع الحد الأدنى للأجور يأتي في إطار الحرص على الاستجابة للمستجدات الاقتصادية الراهنة، بما يعزز الاستقرار الاقتصادي والاجتماعي، مضيفة أن ذلك يتسق مع المعايير الدولية، حيث تؤكد منظمة العمل الدولية على ضرورة مراجعة الحد الأدنى للأجور على أساس دوري، لحماية القوة الشرائية للأسر، واستيعاب التغيرات الاقتصادية التدريجية.\\n\\n## Pydantic Details:\\n{\"properties\": {\"translated_title\": {\"description\": \"Suggested Translated title to the news story.\", \"maxLength\": 300, \"minLength\": 5, \"title\": \"Translated Title\", \"type\": \"string\"}, \"translated_content\": {\"description\": \"The translated content of the news story.\", \"minLength\": 5, \"title\": \"Translated Content\", \"type\": \"string\"}}, \"required\": [\"translated_title\", \"translated_content\"], \"title\": \"TranslatedStory\", \"type\": \"object\"}\\n\\n## Targeted Language or Dialect:\\nEnglish\\n\\n## Translated Story:\\n```json<|im_end|>\\n<|im_start|>assistant\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_model_id = \"news-lora\"\n",
        "\n",
        "llm_response = requests.post(\"http://localhost:8001/v1/completions\", json={\n",
        "    \"model\": vllm_model_id,\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.3\n",
        "})\n",
        "\n",
        "llm_response.json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJV6mATw0m1u",
        "outputId": "023c9b9c-856b-4b3a-c739-91a3ab0a87c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-19a1fcd1c24542d5b589e127f118ab11',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1743636302,\n",
              " 'model': 'news-lora',\n",
              " 'choices': [{'index': 0,\n",
              "   'text': '```json{\"translated_title\": \"Egypt\\'s National Labor Council Increases Minimum Wage to 7,000 EGP\", \"translated_content\": \"The National Labor Council in Egypt has decided to increase the minimum wage for private sector workers to 7,000 Egyptian pounds per month, compared to 6,000 pounds. The increase will be implemented starting March 2025. Additionally, the council has set the minimum value for periodic bonuses for private sector workers at 250 pounds per month for the first time, deciding to establish a minimum wage for temporary work \"part of the time,\" ensuring their wage is not less than 28 pounds net per hour, according to their definition as stated in the Labor Law. Minister of Planning and Economic Development and International Cooperation, Rania Maktaba, stated that raising the minimum wage is in line with the efforts to respond to the current economic developments, aiming to enhance economic and social stability, adding that this aligns with international standards, as the International Labor Organization emphasizes the need to regularly review the minimum wage to protect the purchasing power of households and accommodate gradual economic changes.\"}```',\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop',\n",
              "   'stop_reason': None,\n",
              "   'prompt_logprobs': None}],\n",
              " 'usage': {'prompt_tokens': 476,\n",
              "  'total_tokens': 711,\n",
              "  'completion_tokens': 235,\n",
              "  'prompt_tokens_details': None}}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_model_id = \"news-lora\"\n",
        "\n",
        "llm_response = requests.post(\"http://localhost:8001/v1/completions\", json={\n",
        "    \"model\": vllm_model_id,\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.3\n",
        "})\n",
        "\n",
        "llm_response.json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn56HTkWgRv7",
        "outputId": "2d13eb72-2213-41c2-9dc0-d1cb8bd15ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-a19cb46f9c9e4219862112a397d39195',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1743630987,\n",
              " 'model': 'news-lora',\n",
              " 'choices': [{'index': 0,\n",
              "   'text': '```json{\"translated_title\": \"The Role of Family in Financial Relationships\", \"translated_content\": \"Forbes magazine reported that the family plays a pivotal role in shaping an individual\\'s relationship with money, as this relationship is influenced by inherited financial behaviors across generations.\\\\n\\\\nThe report, based on research by Professor Shane Jensen on financial well-being, explains that each person has a \"financial personality\" that is determined by how they interact with money, which is directly affected by family upbringing and childhood experiences.\\\\n\\\\nThe three dimensions of the financial relationship\\\\nAccording to the study, there are three main dimensions that form our relationship with money:\\\\n\\\\nA: Accumulation (A): Individuals belonging to this dimension tend to view money as a commodity that can be accumulated, seeing wealth accumulation as a goal in itself. The downside of this pattern is the potential for it to turn into an obsession with wealth or the opposite, completely rejecting the idea of earning money as a source of corruption.\\\\n\\\\nU: Utilization (U): These individuals view money as a tool for enjoying life, linking its value to the ability to provide enjoyment and comfort. However, some may become compulsive spenders, while others turn to excessive frugality out of fear for the future.\\\\n\\\\nM: Management (M): Individuals of this type consider money a responsibility that requires meticulous planning. However, in some cases, it may turn into an obsessive preoccupation with managing spending, which negatively affects personal relationships.\\\\n\\\\nHow does family influence our financial relationship?\\\\n\\\\nThe report indicates that family experiences play a crucial role in determining the \"financial personality\" of each individual, for example, if one parent relies on money as a reward for good behavior, the child may later adopt the same pattern in their adult life.\\\\n\\\\nTo analyze these impacts accurately, the Financial Therapy Association developed a tool called the Money Genogram (Financial Genogram), a model used to identify financial patterns within the family.\\\\n\\\\nThis tool includes:\\\\n\\\\nDrawing a family tree.\\\\nClassifying family members according to the three dimensions of the financial relationship (A, U, M).\\\\nDetermining whether the financial behavior of each individual is healthy (+) or unhealthy (-). \\\\nFor example, if a person grew up in a family that was accustomed to excessive spending, they may have a strong inclination to adopt the same pattern, or the opposite, where they become excessively frugal as a psychological reaction.\"}```',\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop',\n",
              "   'stop_reason': None,\n",
              "   'prompt_logprobs': None}],\n",
              " 'usage': {'prompt_tokens': 873,\n",
              "  'total_tokens': 1371,\n",
              "  'completion_tokens': 498,\n",
              "  'prompt_tokens_details': None}}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdUYW1MQ3u94"
      },
      "source": [
        "## Load Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "MmWyu6h3qyJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1794795c-d516-44d7-dc9b-ec302b5c484b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing locust.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile locust.py\n",
        "\n",
        "import random\n",
        "import json\n",
        "from locust import HttpUser, task, between, constant\n",
        "from transformers import AutoTokenizer\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker('ar')\n",
        "# create one user (HttpUser)\n",
        "class CompletionLoadTest(HttpUser):\n",
        "    wait_time = between(1, 3) # wait between each request and the other between 1s to 3s\n",
        "\n",
        "    @task\n",
        "    def post_completion(self):\n",
        "        model_id = \"news-lora\"\n",
        "        prompt = fake.text(max_nb_chars=random.randint(150, 200))\n",
        "\n",
        "        message = {\n",
        "            \"model\": model_id,\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": 512,\n",
        "            \"temperature\": 0.3\n",
        "        }\n",
        "\n",
        "        llm_response = self.client.post(\"/v1/completions\", json=message)\n",
        "\n",
        "        # save llm response in file to ensure the output , but prefered to be in logs not creating files\n",
        "        if llm_response.status_code == 200:\n",
        "            with open(\"./vllm_tokens.txt\", \"a\") as dest:\n",
        "                dest.write(json.dumps({\n",
        "                    \"prompt\": prompt,\n",
        "                    \"response\": llm_response.json()[\"choices\"][0][\"text\"],\n",
        "                }, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!netstat -tuln | grep 8001\n",
        "!ps aux | grep vllm\n",
        "!tail -n 200 nohup.out\n",
        "!tail -n 200 nohup.err"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4osb_DWz6gPR",
        "outputId": "800e480b-cf6c-4a6c-d995-a1e757a56afd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       12329  0.0  0.0   7376  3464 ?        S    23:50   0:00 /bin/bash -c ps aux | grep vllm\n",
            "root       12331  0.0  0.0   6484  2280 ?        S    23:50   0:00 grep vllm\n",
            "INFO 04-02 23:43:35 __init__.py:190] Automatically detected platform cuda.\n",
            "INFO 04-02 23:43:37 api_server.py:840] vLLM API server version 0.7.2\n",
            "INFO 04-02 23:43:37 api_server.py:841] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-1.5B-Instruct', config='', host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='news-lora', path='/gdrive/MyDrive/LLM-Finetunning/models', base_model_name=None)], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-1.5B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.4, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7e3a93b28860>)\n",
            "INFO 04-02 23:43:37 api_server.py:206] Started engine process with PID 10414\n",
            "WARNING 04-02 23:43:40 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 04-02 23:43:46 __init__.py:190] Automatically detected platform cuda.\n",
            "WARNING 04-02 23:43:51 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 04-02 23:43:54 config.py:542] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
            "WARNING 04-02 23:43:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "WARNING 04-02 23:43:54 config.py:678] Async output processing is not supported on the current platform type cuda.\n",
            "INFO 04-02 23:44:02 config.py:542] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
            "WARNING 04-02 23:44:02 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "WARNING 04-02 23:44:02 config.py:678] Async output processing is not supported on the current platform type cuda.\n",
            "INFO 04-02 23:44:02 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
            "INFO 04-02 23:44:04 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 04-02 23:44:04 cuda.py:227] Using XFormers backend.\n",
            "INFO 04-02 23:44:05 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
            "INFO 04-02 23:44:06 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 04-02 23:44:07 weight_utils.py:297] No model.safetensors.index.json found in remote.\n",
            "INFO 04-02 23:44:25 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
            "INFO 04-02 23:44:25 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 04-02 23:44:31 worker.py:267] Memory profiling takes 5.55 seconds\n",
            "INFO 04-02 23:44:31 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.40) = 5.90GiB\n",
            "INFO 04-02 23:44:31 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 1.57GiB.\n",
            "INFO 04-02 23:44:31 executor_base.py:110] # CUDA blocks: 3676, # CPU blocks: 9362\n",
            "INFO 04-02 23:44:31 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 28.72x\n",
            "INFO 04-02 23:44:36 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 11.25 seconds\n",
            "INFO 04-02 23:44:37 api_server.py:756] Using supplied chat template:\n",
            "INFO 04-02 23:44:37 api_server.py:756] None\n",
            "INFO 04-02 23:44:43 serving_models.py:174] Loaded new LoRA adapter: name 'news-lora', path '/gdrive/MyDrive/LLM-Finetunning/models'\n",
            "INFO 04-02 23:44:43 launcher.py:21] Available routes are:\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /openapi.json, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /docs, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /redoc, Methods: GET, HEAD\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /health, Methods: GET\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /ping, Methods: GET, POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /tokenize, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /detokenize, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/models, Methods: GET\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /version, Methods: GET\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/completions, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/embeddings, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /pooling, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /score, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/score, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /rerank, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v1/rerank, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /v2/rerank, Methods: POST\n",
            "INFO 04-02 23:44:43 launcher.py:29] Route: /invocations, Methods: POST\n",
            "INFO:     197.35.97.88:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     197.35.97.88:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO 04-02 23:46:50 logger.py:39] Received request cmpl-c8e937c7d53a4910aae3fe50530ba948-0: prompt: 'فكان اتفاق غينيا اللازمة إيو بتونس بعض. بجسيمة شرسة بفرض قررت الشرقي فصل سبتمبر.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [20931, 124703, 12961, 124880, 124181, 125040, 123860, 124075, 143099, 85153, 125113, 126805, 123890, 20064, 128538, 13, 27846, 33090, 124142, 124325, 52157, 11071, 124966, 27846, 134210, 77703, 11071, 126458, 125249, 123995, 45577, 124224, 59842, 21360, 123978, 83827, 13], lora_request: LoRARequest(lora_name='news-lora', lora_int_id=1, lora_path='/gdrive/MyDrive/LLM-Finetunning/models', lora_local_path=None, long_lora_max_len=None, base_model_name=None), prompt_adapter_request: None.\n",
            "INFO 04-02 23:46:53 logger.py:39] Received request cmpl-c5432ab7bc0040bd93dc32a4c5c63b70-0: prompt: 'عرض دارت مواقعها كل فهرست الدمج وعلى. الشمل ماليزيا شواطيء. وفي للأراضي جُل بحث. بالحرب الطرفين الاندونيسية وتم بحق بأيدي.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [126320, 44330, 129786, 136215, 124006, 128288, 45577, 124190, 46586, 124220, 127641, 131247, 13, 124209, 124014, 23364, 124421, 39697, 124075, 52157, 123920, 125903, 124678, 13, 128641, 130579, 125335, 14558, 82168, 64604, 8532, 27846, 29825, 84532, 13, 124476, 29825, 124011, 17166, 130349, 123860, 125696, 127017, 124392, 73441, 37524, 123978, 27846, 124861, 129521, 133065, 13], lora_request: LoRARequest(lora_name='news-lora', lora_int_id=1, lora_path='/gdrive/MyDrive/LLM-Finetunning/models', lora_local_path=None, long_lora_max_len=None, base_model_name=None), prompt_adapter_request: None.\n",
            "INFO 04-02 23:46:53 logger.py:39] Received request cmpl-1460f6b67e12491f8596f68e84596613-0: prompt: 'الضروري كان ٠٨٠٤ انتصارهم تشيكوسلوفاكيا. حتى باستخدام وبعد كل الطرفين اليها. بالمحور الضروري الدنمارك شرسة.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [31382, 125248, 125729, 126214, 220, 149, 254, 149, 101, 149, 254, 149, 97, 12961, 124126, 129448, 124138, 143700, 126300, 12653, 123993, 124387, 128559, 124075, 13, 128523, 139073, 133593, 128288, 17166, 130349, 123860, 125461, 124006, 13, 124476, 124712, 58656, 17166, 125248, 125729, 124220, 11798, 139583, 52157, 11071, 124966, 13], lora_request: LoRARequest(lora_name='news-lora', lora_int_id=1, lora_path='/gdrive/MyDrive/LLM-Finetunning/models', lora_local_path=None, long_lora_max_len=None, base_model_name=None), prompt_adapter_request: None.\n",
            "INFO 04-02 23:46:53 logger.py:39] Received request cmpl-62daad2890544e498ac5e29a41dfc844-0: prompt: 'اتفاقية يبق لتقليعة يعادل. الشتاء الإقتصادية كانت المتّبعة للسيطرة. حقول فكان والتي الصين مئات الصين الى استرجاع.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [127394, 73441, 73274, 125173, 56794, 139208, 124511, 73274, 129714, 8532, 13, 124209, 14293, 98719, 124058, 27490, 124567, 133642, 128671, 128631, 73771, 21360, 124511, 125006, 133433, 25871, 13, 68238, 126311, 45577, 124703, 129833, 132781, 23364, 124082, 47632, 132781, 128470, 93153, 126415, 123862, 13], lora_request: LoRARequest(lora_name='news-lora', lora_int_id=1, lora_path='/gdrive/MyDrive/LLM-Finetunning/models', lora_local_path=None, long_lora_max_len=None, base_model_name=None), prompt_adapter_request: None.\n",
            "INFO 04-02 23:46:54 logger.py:39] Received request cmpl-956941600d674a478a58cb2326c7b2fa-0: prompt: 'فعل المضي الثالث ويعزى. البشريةً بدول جديدة معاملة التي النزاع. ب٠٨٠٤ التي شعار جمعت القوى.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [130029, 53479, 57859, 14558, 132607, 37524, 124280, 39697, 55057, 13, 132012, 73441, 124376, 27846, 131499, 130282, 126196, 124466, 25871, 128261, 124080, 39697, 123862, 13, 27846, 149, 254, 149, 101, 149, 254, 149, 97, 128261, 52157, 125164, 82168, 124363, 14293, 124172, 125187, 13], lora_request: LoRARequest(lora_name='news-lora', lora_int_id=1, lora_path='/gdrive/MyDrive/LLM-Finetunning/models', lora_local_path=None, long_lora_max_len=None, base_model_name=None), prompt_adapter_request: None.\n",
            "ERROR 04-02 23:46:54 client.py:300] RuntimeError('Engine process (pid 10414) died.')\n",
            "ERROR 04-02 23:46:54 client.py:300] NoneType: None\n",
            "CRITICAL 04-02 23:46:55 launcher.py:101] MQLLMEngine is already dead, terminating server process\n",
            "INFO:     34.124.187.102:0 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\n",
            "CRITICAL 04-02 23:47:04 launcher.py:101] MQLLMEngine is already dead, terminating server process\n",
            "INFO:     34.124.187.102:0 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\n",
            "CRITICAL 04-02 23:47:04 launcher.py:101] MQLLMEngine is already dead, terminating server process\n",
            "INFO:     34.124.187.102:0 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\n",
            "CRITICAL 04-02 23:47:04 launcher.py:101] MQLLMEngine is already dead, terminating server process\n",
            "INFO:     34.124.187.102:0 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\n",
            "CRITICAL 04-02 23:47:04 launcher.py:101] MQLLMEngine is already dead, terminating server process\n",
            "INFO:     34.124.187.102:0 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\n",
            "CRITICAL 04-02 23:47:04 launcher.py:101] MQLLMEngine is already dead, terminating server process\n",
            "INFO:     34.124.187.102:0 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.87s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.87s/it]\n",
            "\n",
            "INFO:     Started server process [10316]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for connections to close. (CTRL+C to force quit)\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [10316]\n",
            "/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!locust --headless -f locust.py --host=http://127.0.0.1:8001 -u 20 -r 1 -t \"60s\" --html=locust_results.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmHmKkpS6Gpd",
        "outputId": "47cd1b88-2f02-414a-e801-38a4b721ae19"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-02 23:49:12,713] 002210d31c41/INFO/locust.main: Starting Locust 2.33.2\n",
            "[2025-04-02 23:49:12,714] 002210d31c41/INFO/locust.main: Run time limit set to 60 seconds\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated       0     0(0.00%) |      0       0       0      0 |    0.00        0.00\n",
            "\n",
            "[2025-04-02 23:49:12,715] 002210d31c41/INFO/locust.runners: Ramping to 20 users at a rate of 1.00 per second\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions       2   2(100.00%) |      4       1       6      2 |    0.00        0.00\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated       2   2(100.00%) |      4       1       6      2 |    0.00        0.00\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions       6   6(100.00%) |      2       1       6      2 |    1.00        1.00\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated       6   6(100.00%) |      2       1       6      2 |    1.00        1.00\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions      12  12(100.00%) |      1       0       6      2 |    1.25        1.25\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated      12  12(100.00%) |      1       0       6      2 |    1.25        1.25\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions      21  21(100.00%) |      1       0       6      1 |    1.67        1.67\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated      21  21(100.00%) |      1       0       6      1 |    1.67        1.67\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions      31  31(100.00%) |      1       0       6      1 |    2.12        2.12\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated      31  31(100.00%) |      1       0       6      1 |    2.12        2.12\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions      44  44(100.00%) |      1       0       6      1 |    2.90        2.90\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated      44  44(100.00%) |      1       0       6      1 |    2.90        2.90\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions      56  56(100.00%) |      1       0       6      1 |    3.70        3.70\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated      56  56(100.00%) |      1       0       6      1 |    3.70        3.70\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions      74  74(100.00%) |      1       0       6      1 |    4.90        4.90\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated      74  74(100.00%) |      1       0       6      1 |    4.90        4.90\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions      94  94(100.00%) |      1       0       6      1 |    6.00        6.00\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated      94  94(100.00%) |      1       0       6      1 |    6.00        6.00\n",
            "\n",
            "[2025-04-02 23:49:31,729] 002210d31c41/INFO/locust.runners: All users spawned: {\"CompletionLoadTest\": 20} (20 total users)\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     111 111(100.00%) |      1       0       6      1 |    6.60        6.60\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     111 111(100.00%) |      1       0       6      1 |    6.60        6.60\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     130 130(100.00%) |      1       0       6      1 |    7.60        7.60\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     130 130(100.00%) |      1       0       6      1 |    7.60        7.60\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     152 152(100.00%) |      1       0       6      1 |    8.40        8.40\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     152 152(100.00%) |      1       0       6      1 |    8.40        8.40\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     171 171(100.00%) |      1       0       6      1 |    8.90        8.90\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     171 171(100.00%) |      1       0       6      1 |    8.90        8.90\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     191 191(100.00%) |      1       0       6      1 |    9.40        9.40\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     191 191(100.00%) |      1       0       6      1 |    9.40        9.40\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     213 213(100.00%) |      1       0       6      1 |   10.20       10.20\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     213 213(100.00%) |      1       0       6      1 |   10.20       10.20\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     231 231(100.00%) |      1       0       6      1 |    9.90        9.90\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     231 231(100.00%) |      1       0       6      1 |    9.90        9.90\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     254 254(100.00%) |      1       0       6      1 |   10.30       10.30\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     254 254(100.00%) |      1       0       6      1 |   10.30       10.30\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     270 270(100.00%) |      1       0       6      1 |   10.40       10.40\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     270 270(100.00%) |      1       0       6      1 |   10.40       10.40\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     289 289(100.00%) |      1       0       6      1 |   10.10       10.10\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     289 289(100.00%) |      1       0       6      1 |   10.10       10.10\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     311 311(100.00%) |      1       0       6      1 |    9.80        9.80\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     311 311(100.00%) |      1       0       6      1 |    9.80        9.80\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     331 331(100.00%) |      1       0       6      1 |   10.30       10.30\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     331 331(100.00%) |      1       0       6      1 |   10.30       10.30\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     347 347(100.00%) |      1       0       6      1 |    9.70        9.70\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     347 347(100.00%) |      1       0       6      1 |    9.70        9.70\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     371 371(100.00%) |      1       0       6      1 |    9.60        9.60\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     371 371(100.00%) |      1       0       6      1 |    9.60        9.60\n",
            "\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     391 391(100.00%) |      1       0       6      1 |    9.80        9.80\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     391 391(100.00%) |      1       0       6      1 |    9.80        9.80\n",
            "\n",
            "[2025-04-02 23:50:01,913] 002210d31c41/INFO/locust.main: --run-time limit reached, shutting down\n",
            "[2025-04-02 23:50:02,026] 002210d31c41/INFO/locust.main: writing html report to file: locust_results.html\n",
            "[2025-04-02 23:50:02,029] 002210d31c41/INFO/locust.main: Shutting down (exit code 1)\n",
            "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "POST     /v1/completions     403 403(100.00%) |      1       0       6      1 |    8.21        8.21\n",
            "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
            "         Aggregated     403 403(100.00%) |      1       0       6      1 |    8.21        8.21\n",
            "\n",
            "Response time percentiles (approximated)\n",
            "Type     Name      50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
            "--------||--------|------|------|------|------|------|------|------|------|------|------|------\n",
            "POST     /v1/completions        1      1      1      1      1      2      2      2      7      7      7    403\n",
            "--------||--------|------|------|------|------|------|------|------|------|------|------|------\n",
            "         Aggregated        1      1      1      1      1      2      2      2      7      7      7    403\n",
            "\n",
            "Error report\n",
            "# occurrences      Error                                                                                               \n",
            "------------------|-------------------------------------------------------------\n",
            "403                POST /v1/completions: ConnectionRefusedError(111, 'Connection refused')                             \n",
            "------------------|-------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "all",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "196499a676fa4d71a1f312e02a05bc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a429826c6e3b41039c2513101602ab2c",
              "IPY_MODEL_f8343ca0c8604c27a75a823aa7326d20",
              "IPY_MODEL_eee8dfb45af445f08d568e122c72fc25"
            ],
            "layout": "IPY_MODEL_9f1c73a4077e40cbbc2dd074a714e726"
          }
        },
        "253c8e33f63b4ba8a507fc3392761f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64cc8430948f44ddb89a12f4ec2fc1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d3ce2b42c2a4635ae95b1981bfbb8a4",
            "placeholder": "​",
            "style": "IPY_MODEL_df3e428ca22643d79e7420a106b6315a",
            "value": "100%"
          }
        },
        "7229157e2f2845f6bd99d79a602298c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77c974eb60e24d038b538ede01e8a10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64cc8430948f44ddb89a12f4ec2fc1a9",
              "IPY_MODEL_f936f4c117cd42a18bb705fa85164c39",
              "IPY_MODEL_fc3c72bd7dc2486b88d1a2e72e937bd1"
            ],
            "layout": "IPY_MODEL_9b8bbc2772444e68b775ee8ce8d29582"
          }
        },
        "7daebe4cb2ca4741b8040f37b28547d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8391347738f24d6c8f9f08396ab2963f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d3ce2b42c2a4635ae95b1981bfbb8a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b8bbc2772444e68b775ee8ce8d29582": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f1c73a4077e40cbbc2dd074a714e726": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a429826c6e3b41039c2513101602ab2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcf69a747ed44917904e8a03821b3bb0",
            "placeholder": "​",
            "style": "IPY_MODEL_7229157e2f2845f6bd99d79a602298c3",
            "value": "  0%"
          }
        },
        "d6ce49d8080a441ca3e58b939172457f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2c535889fa41eaa5bc7e47246a5e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcf69a747ed44917904e8a03821b3bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df3e428ca22643d79e7420a106b6315a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e49240adb2fe45568210e762b4450404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eee8dfb45af445f08d568e122c72fc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ce49d8080a441ca3e58b939172457f",
            "placeholder": "​",
            "style": "IPY_MODEL_da2c535889fa41eaa5bc7e47246a5e07",
            "value": " 12/2400 [00:40&lt;2:07:35,  3.21s/it]"
          }
        },
        "f8343ca0c8604c27a75a823aa7326d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7daebe4cb2ca4741b8040f37b28547d4",
            "max": 2400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8391347738f24d6c8f9f08396ab2963f",
            "value": 12
          }
        },
        "f8e24e22227745bea0607d53bcadb26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f936f4c117cd42a18bb705fa85164c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_253c8e33f63b4ba8a507fc3392761f6d",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8e24e22227745bea0607d53bcadb26c",
            "value": 30
          }
        },
        "fc3c72bd7dc2486b88d1a2e72e937bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe8001f926834a40bf4039814fcd12e8",
            "placeholder": "​",
            "style": "IPY_MODEL_e49240adb2fe45568210e762b4450404",
            "value": " 30/30 [14:21&lt;00:00, 28.43s/it]"
          }
        },
        "fe8001f926834a40bf4039814fcd12e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95ed09cc5c104cb7ba77a12a9563c230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a8f2cca2e12459f9fda5612e63752ff",
              "IPY_MODEL_27815451e7394f2a881b28d2cd02f2b6",
              "IPY_MODEL_b3d19e4074e84a568eb1615c79ebe15c"
            ],
            "layout": "IPY_MODEL_94da70dd7c1a4b4fb2a937091398e24a"
          }
        },
        "0a8f2cca2e12459f9fda5612e63752ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_982e7af5e0684f9189e359faa1537a42",
            "placeholder": "​",
            "style": "IPY_MODEL_942e45351f8a4a45b8c1c094d83ab2a2",
            "value": "config.json: 100%"
          }
        },
        "27815451e7394f2a881b28d2cd02f2b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41bf4f8f3fee4d30b4e8dc652151c82e",
            "max": 660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb0f0b8d3f874694a68a5dfe894844ce",
            "value": 660
          }
        },
        "b3d19e4074e84a568eb1615c79ebe15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd364054ad994f989582f76cbc126412",
            "placeholder": "​",
            "style": "IPY_MODEL_9d6b672eb1c3490bb989cf24df3182c8",
            "value": " 660/660 [00:00&lt;00:00, 47.2kB/s]"
          }
        },
        "94da70dd7c1a4b4fb2a937091398e24a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "982e7af5e0684f9189e359faa1537a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "942e45351f8a4a45b8c1c094d83ab2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41bf4f8f3fee4d30b4e8dc652151c82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0f0b8d3f874694a68a5dfe894844ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd364054ad994f989582f76cbc126412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d6b672eb1c3490bb989cf24df3182c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1116fb6241db4b2182b3690f7361877b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c38c7244a7864c14907c9c2d888b27fa",
              "IPY_MODEL_ae6cd78d1c57464aade88be1b70f4441",
              "IPY_MODEL_1c80f0b368ea4718af70110ea36d6b80"
            ],
            "layout": "IPY_MODEL_037cab93391f4b34b2d66c4b6fd70468"
          }
        },
        "c38c7244a7864c14907c9c2d888b27fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ce935c0041408f8442f9970380e108",
            "placeholder": "​",
            "style": "IPY_MODEL_dcbadfd14e6d481e91539946417e581c",
            "value": "model.safetensors: 100%"
          }
        },
        "ae6cd78d1c57464aade88be1b70f4441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_923aa191171d4b79b717a1bf8f6f5020",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9ea8ff7a0594b66858164080f163a5c",
            "value": 3087467144
          }
        },
        "1c80f0b368ea4718af70110ea36d6b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0443fc01e664fc983ae11c96f06b46a",
            "placeholder": "​",
            "style": "IPY_MODEL_e9b7d9e50f3b4e9d8ff93e88ccc93bb6",
            "value": " 3.09G/3.09G [00:15&lt;00:00, 256MB/s]"
          }
        },
        "037cab93391f4b34b2d66c4b6fd70468": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ce935c0041408f8442f9970380e108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcbadfd14e6d481e91539946417e581c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "923aa191171d4b79b717a1bf8f6f5020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9ea8ff7a0594b66858164080f163a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0443fc01e664fc983ae11c96f06b46a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9b7d9e50f3b4e9d8ff93e88ccc93bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37b270195dd14976af1328b082466743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5608ea88d584555bfda4b5f02d75899",
              "IPY_MODEL_0178074f03eb43d095997f04fccb742f",
              "IPY_MODEL_fa0fbd4f6e6c4696967ed48d94cfdda8"
            ],
            "layout": "IPY_MODEL_56950b2c2b5042378f8682b883ab34d1"
          }
        },
        "c5608ea88d584555bfda4b5f02d75899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff22ebe5534d408ebc80046d3f3a9604",
            "placeholder": "​",
            "style": "IPY_MODEL_d6e94dd9012745eabc0d713c34d9c660",
            "value": "generation_config.json: 100%"
          }
        },
        "0178074f03eb43d095997f04fccb742f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b69bdc55085f4bbcbbc10f5657a67919",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73ccd1070121494cbe3a5165959061f3",
            "value": 242
          }
        },
        "fa0fbd4f6e6c4696967ed48d94cfdda8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_babdc45323674142aea58f5bcfce51ea",
            "placeholder": "​",
            "style": "IPY_MODEL_b44a271c0606439dbee0ef9481089c4f",
            "value": " 242/242 [00:00&lt;00:00, 27.2kB/s]"
          }
        },
        "56950b2c2b5042378f8682b883ab34d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff22ebe5534d408ebc80046d3f3a9604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e94dd9012745eabc0d713c34d9c660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b69bdc55085f4bbcbbc10f5657a67919": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ccd1070121494cbe3a5165959061f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "babdc45323674142aea58f5bcfce51ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b44a271c0606439dbee0ef9481089c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ed19de0e4484fcc811af8f238346d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_423fbd8e3bf942339803c81f0bbc48a3",
              "IPY_MODEL_7a235530e4d14c88b5ddd59ff6962b27",
              "IPY_MODEL_71aff970b5614fcc960ed2a626205b59"
            ],
            "layout": "IPY_MODEL_913d3e8d27cd4cbea3d8c1827d1d5b93"
          }
        },
        "423fbd8e3bf942339803c81f0bbc48a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57e962feaf514798a97ccd5ede63a5f1",
            "placeholder": "​",
            "style": "IPY_MODEL_0b77e4b586044aca836c58e9bad9bbac",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7a235530e4d14c88b5ddd59ff6962b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5abffac868f40648c70d310ddc832a5",
            "max": 7305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d9dbf1b880647cc83ac8831843cab62",
            "value": 7305
          }
        },
        "71aff970b5614fcc960ed2a626205b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0aaf2f49f8c4296bde0e81b4d7ce7ad",
            "placeholder": "​",
            "style": "IPY_MODEL_3ad41c46102a4b56a10b2c1157844848",
            "value": " 7.30k/7.30k [00:00&lt;00:00, 662kB/s]"
          }
        },
        "913d3e8d27cd4cbea3d8c1827d1d5b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57e962feaf514798a97ccd5ede63a5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b77e4b586044aca836c58e9bad9bbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5abffac868f40648c70d310ddc832a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9dbf1b880647cc83ac8831843cab62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0aaf2f49f8c4296bde0e81b4d7ce7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad41c46102a4b56a10b2c1157844848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8ea807f74484c92a0c979ddf470605f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e0fa50fd2104a93a2ca9f86f5045e43",
              "IPY_MODEL_56266085a47748e2a48e622e28ac7d2c",
              "IPY_MODEL_7a5b8cbd5cf344e9b7e3760203d0db9b"
            ],
            "layout": "IPY_MODEL_6087082f014b474c9f27b75825e9b447"
          }
        },
        "9e0fa50fd2104a93a2ca9f86f5045e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61029ab2ef6e4a149bb8d4cbc67ba02a",
            "placeholder": "​",
            "style": "IPY_MODEL_fe28717fb023455b90ffcbafc241a2ae",
            "value": "vocab.json: 100%"
          }
        },
        "56266085a47748e2a48e622e28ac7d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a95be6366d40ca92d42768947e7725",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a099c34d8b7d4e47adfc38ab2b299383",
            "value": 2776833
          }
        },
        "7a5b8cbd5cf344e9b7e3760203d0db9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3be8cbc9fe94b15949b1743f1e9e855",
            "placeholder": "​",
            "style": "IPY_MODEL_3653c55f7f114385858ac6e809107768",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 4.28MB/s]"
          }
        },
        "6087082f014b474c9f27b75825e9b447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61029ab2ef6e4a149bb8d4cbc67ba02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe28717fb023455b90ffcbafc241a2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19a95be6366d40ca92d42768947e7725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a099c34d8b7d4e47adfc38ab2b299383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3be8cbc9fe94b15949b1743f1e9e855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3653c55f7f114385858ac6e809107768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03bbcada456c4f5d94f1766c0853475f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91b86e78252b443b9d874723a55552ff",
              "IPY_MODEL_fefcb96a851c4ebeaacb2a1238518b18",
              "IPY_MODEL_28e654f30dbf425185f010f829f78ff6"
            ],
            "layout": "IPY_MODEL_749c927e8fe749f8b72e6a89b79abab1"
          }
        },
        "91b86e78252b443b9d874723a55552ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8c9c010d10d49d2963ebba06d66cf76",
            "placeholder": "​",
            "style": "IPY_MODEL_6c3414d2ef664a6bad5d889be17721ec",
            "value": "merges.txt: 100%"
          }
        },
        "fefcb96a851c4ebeaacb2a1238518b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f70917bb9194311a25b98bc3002b65f",
            "max": 1671839,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90ac7cd7b35c43a19312f878d9193759",
            "value": 1671839
          }
        },
        "28e654f30dbf425185f010f829f78ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d584d627a8462cb0e60c3895440040",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c6759e9f74405bb235a8e8ba3c7eb7",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 18.9MB/s]"
          }
        },
        "749c927e8fe749f8b72e6a89b79abab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c9c010d10d49d2963ebba06d66cf76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c3414d2ef664a6bad5d889be17721ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f70917bb9194311a25b98bc3002b65f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ac7cd7b35c43a19312f878d9193759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00d584d627a8462cb0e60c3895440040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c6759e9f74405bb235a8e8ba3c7eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c3f9feb9a844024824a3ed71ab58694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c1a265e316a485d814b8837cd064dcd",
              "IPY_MODEL_bd199efff79c46a9a93be41a52aa656e",
              "IPY_MODEL_a529fbac3a5741be84223f8cfb643833"
            ],
            "layout": "IPY_MODEL_3e1b008e0ce441dc9809133141cb05b5"
          }
        },
        "3c1a265e316a485d814b8837cd064dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8649e903b174da88a6c9eefaf88c307",
            "placeholder": "​",
            "style": "IPY_MODEL_016fb16a7c5a4076b7e371f48afd2455",
            "value": "tokenizer.json: 100%"
          }
        },
        "bd199efff79c46a9a93be41a52aa656e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e16ce01eb3468897951ce6aa1cdb3c",
            "max": 7031645,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af02ee05f7c5408ea4dfaac442548790",
            "value": 7031645
          }
        },
        "a529fbac3a5741be84223f8cfb643833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3da0666d8ebe49058c134fcea672f379",
            "placeholder": "​",
            "style": "IPY_MODEL_4932532b79ba4defb2d1d57e1a6c7a3e",
            "value": " 7.03M/7.03M [00:00&lt;00:00, 14.8MB/s]"
          }
        },
        "3e1b008e0ce441dc9809133141cb05b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8649e903b174da88a6c9eefaf88c307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "016fb16a7c5a4076b7e371f48afd2455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87e16ce01eb3468897951ce6aa1cdb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af02ee05f7c5408ea4dfaac442548790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3da0666d8ebe49058c134fcea672f379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4932532b79ba4defb2d1d57e1a6c7a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}